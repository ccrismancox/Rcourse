\documentclass[12pt]{report}
\author{Casey Crisman-Cox\footnote{This document contains some material (including some chunks of code that are lifted verbatim) from Peter Haschke's \emph{An Introduction to \R} and Brenton Kenkel's \emph{An Introduction to \R}, under the terms of the Creative Commons Attribution license. Both of their works are available on the Star Lab website, \url{http://www.rochester.edu/college/psc/thestarlab/resources}. All errors are my own.  }}
\title{\R\ Introduction}
\date{\today}
\usepackage[top=1.00in,bottom=1.00in,left=1.00in,right=1in]{geometry}
\usepackage{amsmath, amsfonts, amsthm, setspace, fancyhdr, natbib, url, array, float, graphicx, color, verbatim, subfig}
\usepackage{rotating}
\usepackage[margin=1cm]{caption}
\usepackage{hyperref}
\hypersetup{colorlinks   = true}
\hypersetup{pdfborder = 0 0 0}
\usepackage{ragged2e}
\usepackage{tikz}
\newcommand{\R}{\textsf{R}}
\newtheorem*{note}{Note}
\begin{document}
%\SweaveOpts{concordance=TRUE}
%\SweaveOpts{concordance=TRUE}
\maketitle
\newpage
This document is released under the Creative Commons Attribution license â€“
\url{http://creativecommons.org/licenses/by/3.0/}
\tableofcontents
<<echo=FALSE, results="hide">>=
knitr::opts_chunk$set(comment=NA)
knitr::opts_chunk$set(prompt=TRUE, tidy=FALSE)
options(keep.blank.lines= TRUE)
setwd('~')
@

\chapter{The Basics of \R}
\section{What is \R}
You've already decided to learn \R\   so I don't need to write the congratulatory paragraph that opens nearly every \R\ tutorial.
But I will say a few nice things about \R.  Some of the things that \R\  is good at
\begin{itemize}
  \item New methods are frequently released with an \R\  package or \R\  code.
	\begin{itemize}
		\item If new methods don't come with code you can write it yourself in \R.
	\end{itemize}
	\item Methods like strategic estimators are, to my knowledge, not readily available in Stata, whereas they are straight forward in \R.
	\item I personally find data management easier to do in \R.
	\item \R\ plots are easy on the eyes.
\end{itemize}

\section{Course Aims and Structure}
At the end of course sessions you should be able to 
\begin{itemize}
	\item Install/Update \R\ and \R\ packages (1)
	\item Know where to look for \R\ help (1)
	\item Use \R\ to read and save data  (3)
	\item Effectively use matrices (1) and data frames (3) in \R\   %Cover if else
  \item Use control statements to program iterative procedures (2)
	\item Conduct basic statistical analysis with \R\ (5)
	\item Create tables (5) and plots (4) that can be exported directly into \LaTeX 
	\item Create simple programs using \R\  (1-2) %Assignment after 3 (create a likelihood ratio text that returns results as a list and then as a matrix) 
													 %Run 2 small models and one big model.  Put the small models into a list.  Use both for and lapply to run your LRtest against the full model.
													 %Using a while loop create an iterative logit return as a list
\end{itemize}
We should be able to cover all this in 4 or 5 sessions, each one lasting no more than an hour.
Today we'll just look at installing \R\ and \R\ packages, \R\ help, and some basic operations with vectors and matrices.
\section{Installing \R}

To install \R\ for Windows
\begin{enumerate}
  \item Go to \url{http://cran.r-project.org/}
	\item Click on ``Download \R\ for Windows''
				\begin{figure}[H]
					\centering
					\includegraphics[scale=0.5]{cran1.png}
					\caption{\url{http://cran.r-project.org/}}
					\label{CRAN1}
				\end{figure}
	\item Click on ``base''
			\begin{figure}[H]
				\centering
				\includegraphics[scale=0.5]{cran2.png}
				\caption{\url{http://cran.r-project.org/bin/windows/}}
				\label{CRAN2}
			\end{figure}
	\item Finally click on the big button download at the top of the page and run the file that it downloads
			\begin{figure}[H]
				\centering
				\includegraphics[scale=0.45]{cran3.png}
				\caption{\url{http://cran.r-project.org/bin/windows/base}}
				\label{CRAN3}
			\end{figure}
\end{enumerate} 
You how have \R\ installed on your computer. 

To install \R\ on a Mac is largely the same.
\begin{enumerate}
  \item Go to \url{http://cran.r-project.org/}
	\item Click on ``Download \R\ for (Mac OS X)''
				\begin{figure}[H]
					\centering
					\includegraphics[scale=0.5]{cran1.png}
					\caption{\url{http://cran.r-project.org/}}
				\end{figure}
	\item Click on the version that matches your Mac
			\begin{figure}[H]
				\centering
				\includegraphics[scale=0.5]{cranMAC.png}
				\caption{\url{http://cran.r-project.org/bin/macosx/}}
			\end{figure}
\end{enumerate} 
You how have \R\ installed on your computer. 

For Linux users you'll want to follow click on ``Download \R\ for Linux'' find your distribution and follow the instructions.  
This is a little less straight forward than with Windows and Mac, but if you run into any trouble email me.

When you've finished installing \R\ open it up you should see something that looks like this:
\begin{figure}[H]
\centering
\includegraphics[trim=0 0 0 0, scale=0.7]{Rconsole.png}
\caption{R console}
\label{Rcon}
\end{figure}

As you can see in the picture, this version of \R\ is version 3.1.2, if we wanted more information about the type of \R\ we're running we can use the command \verb@sessionInfo()@ and we get
<<>>=
sessionInfo()
@
Which shows us the version of \R\ we're using, our operating system, and the packages we currently have loaded.   
Since you haven't loaded any packages yet, the packages listed  are those that that \R\ loads automatically each time it opens.  
\begin{note}
In the above chunk I have the \verb@knitr@ package loaded, which means my output has ``other attached packages'' and ``loaded via namespace'' your output will not have that.
\end{note}

\R\ Studio is an excellent alternative to the \R\ console as it provides a nice system to edit your files while you're working on them and keep everything better organized. 
To download \R\ Studio visit \url{http://www.rstudio.com/products/rstudio/download/} and find the installer that matches your system.
I strongly recommend the use of \R\ Studio over the regular \R\ console for ease of use and organization.


\section{Using \R\ as a Calculator}
Now that we've gone through that ordeal, let's actually use \R\ for something.
When we open up \R\ we have the rather intimidating looking prompt staring at us.  Whenever we see 
<<prompt=FALSE, eval=FALSE>>=
>
@
It just means that \R\ is waiting for us to give it something to do.  Let's start with something simple
<<>>=
1+1
@
Which gives our answer and returns us to the \verb@>@.  Now we don't have to fit everything on one line.  If we don't type a full command \R\ changes the \verb@>@ to a \verb@+@ to let us know that it needs more from us.  For example:
<<tidy=FALSE>>=
2*
2
@
If for some reason you get the \verb@+@ and you don't know what went wrong you can hit the escape button on your keyboard and that stops \R\  and returns you to the \verb@>@.  Escape will terminate anything \R\ is doing and return you to the \verb@>@ prompt.
\\[12pt]
All the basic operations work in \R\ so \verb@+, -, *, /, ^@ do addition, subtraction, multiplication, division, and exponents just as we would expect them to do.  
Additionally, standard functions are available so:
<<>>=
log(10)  #base= e
log(10, base=10)  
exp(1)
sin(0)
acos(-1)
@
Note that \verb@#@ is how we use comments in \R.  A comment is just a remark we put with our code but don't want \R\ to evaluate.  
So after the \verb@#@ \R\ stops reading the line.\\[12pt]
Also, \R\ can't do the impossible so 
<<>>=
log(0)
log(-1)
@
Where \verb@-Inf@ means $-\infty$ and \verb@NaN@ means ``Not a Number''.  Getting those is a sign that you need to reevaluate what you're doing.
\section{Vectors and Variables}
Now we want to use \R\ for more than just a calculator (your computer already has one of those).   So now we want to expand what we can do, the first way we'll do that is by assigning the output of our calculations to a variable.  
In \R, an assignment can take many forms, and all of the following are the same.
<<>>=
x <- exp(1)
x = exp(1)
exp(1) -> x
assign('x', exp(1))
@
For the most part, you'll only ever see the first two, and most \R\ users prefer the \verb@<-@.
Once a value is assigned to variable we can use \verb@x@ like any other number and so
<<>>=
x
x-2
log(x)
@
If we want to assign a new value to \verb@x@  we just use the arrow again
<<>>=
x <- exp(2)
x
@
\subsection{Naming Variables}
We can name variables anything. Within code it is often better to use descriptive names.  The only rules about naming  variable is that it can't start with a number or contain any symbols except for periods and underscores.
<<tidy=FALSE>>=
n <- 50 #Good but not descriptive
numberOfStates <- 50 #Good and descriptive
number.of.states <- 50 #Still good
number_of_states <- 50 #Still good
number-of-states <- 50 #Not good
@
As you can see the last one returned an error.  Using dashes made \R\ think we wanted to subtract the variable \verb@number@ minus the variable \verb@of@ minus the variable \verb@students@.  If these variables had existed we would have gotten a different error because \R\ would think we wanted to assign the value 10 to this difference, which it would say is nonsense.\\[12pt]
Notice that all of our output began with the symbol \verb@[1]@, for example
<<>>=
2+2 
@
The \verb@[1]@ just means that \R\ thinks of this as a vector and the the \verb@[1]@ just tells you that the value next to it is the first number in the vector.
There's no reason why a variable in \R\ has to have only one value.  The simplest way to create vector is with the \verb@c()@ function.  For example
<<>>=
x1 <- c(1, 2, 3, 4)
x1
@
Notice that the \verb@[1]@ is still there to tell us that the number next to it is the first value in the vector.
The \verb@c@ in this function just stands for ``concatenate'' and it can  be used to bring lots of vectors  together
<<>>=
x2 <- c(1, 0, -1, 1)
c(x2, x2, x2, x1, x1, x1, x2, x2, x1, x1)
@
Where we can now see that whenever the output goes onto a second line we get a new indicator to tell us what position it is.  So in the above we have \verb@[1]@ at the beginning of the output and then \verb@[36]@ to tell us the value that starts the second line is the 36th value in the vector. \\[12pt]
Nearly all the functions we looked at before work on vectors.  For instance
<<>>=
x1+x2
x1/x2
log(x1)
@
And there are some nice functions to describe vectors.
<<>>=
sum(x1)
prod(x1)
mean(x1)
median(x1)
sd(x1)
@
We can also sort the values within a vector
<<>>=
sort(x1)
sort(x1, decreasing=TRUE)
length(x1)
@
\subsection{Easier ways to Create Vectors}
If we want to create a vector that follows a pattern, we don't need to take the time to type it in. 
For instance if we just want all the numbers between 1 and 15 in a vector we can use the colon.
<<>>=
1:15
5:2
@
Notice that \R\  reads the second one as a sequence from 5 to 2, and so it goes in decreasing order.  The more general version of the colon is the \verb@seq()@ command
<<>>=
seq(0, 20)
seq(0, 20, by=2)
seq(0, 20, length.out=5)
@
Finally  the \verb@rep@ command allows you to repeat numbers
<<>>=
rep(10, 2)
rep(x1, 3)
rep(x1, each=3) #Repeats each number within x1 one at a time
@	
\subsection{Indexing}
Let's say we want to extract or replace a single number within a vector. In these cases we use the square brackets, for example
<<>>=
z <- seq(0, 6, by =2)
z[3] #3rd entry
z[1:3] #1st three entries
z[c(1, 3)] #Entries 1 and 3, note that we need c()
z[-c(1,3)] #Everything but 1 and 3
@
We can also extract based on a pattern using logical operators.  Let's say we only want elements of \verb@z@ that are greater than 10. The logical statement is 
<<>>=
z >3
@
Which returns a vector of \verb@TRUE@ and \verb@FALSE@ values to show if a particular element in \verb@z@ meets the condition we gave it.  Now in order to use that to get the elements we want do the following:
<<>>=
z[z>3]
@
The  list of commonly used logical operators  is shown in table \ref{LO}
\begin{table}[H]
\centering
\caption{Logical operators}
\label{LO}
\begin{tabular}{cl}
Operator  	& Meaning\\ \hline
\verb@<@					 		& less than\\
\verb@<=@					& less than or equal to\\
\verb@> @						& greater than\\
\verb@>=@					& greater than or equal to \\
\verb@==@					& equal \\
\verb@!=@						& Not equal\\
\verb@!@							& Not\\ \hline
\end{tabular}				
\end{table}
Logical conditions can be strung together use \verb@&@ (and) and \verb@|@ or
<<>>=
z > 3 & z< 5
z[z > 3 & z < 5]
z[z < 3 | z > 5]
@
\subsection{Removing Objects}	
We use the \verb@ls()@ command to view all the objects that we've created
<<>>=
ls()
@
Now lets say we wanted to get rid of some things.  For this we use the \verb@rm()@ command, but be careful, there's no undo for this.
<<>>=
rm(list='number.of.states')
ls()
rm(list=c('x1', 'y2')) #We can delete more than one thing at time.
ls()
rm(list=ls()) #We can delete everything
ls()
@
It's worth noting at this point that a vector doesn't have to be numbers it could be 
<<>>=
x <- c('cat', 'dog', 'horse')
@
Until we get more into data analysis there isn't a whole lot of reason to get into strings. 
I will note that the \verb@stringr@ package contains many good tools for manipulating string variables should you find yourself needing to do that.
\section{Matrices}
A matrix is just a 2 dimensional version of the vector.  To create a matrix you just need a vector of values and then tell \R\ one of the dimensions
<<>>=
x <- 1:10
matrix(x, nrow=2)
matrix(x, ncol=2)
@
Notice that \R\ fills in the numbers column-wise, but we can also fill in row wise
<<>>=
matrix(x, ncol=2, byrow=TRUE)
@
We can also use \verb@cbind@ and \verb@rbind@ to ``bind''  vectors together to make a matrix, bind a vector(s) to a matrix, or bind matrices together
<<>>=
x2 <- -10:-1
cbind(x, x2)
rbind(x, x2)
z <- 1:5
cbind(x, x2, z)
@
Notice that there's no limit to the number of things we can bind together in one use of \verb@cbind@.\\[12pt]
The \verb@diag@ command has a few different uses.  
<<>>=
diag(4) # 4 x 4 identity matrix 
diag(x) #A square matrix with diagonal = x
Z <- matrix(1:9, nrow = 3)
Z
diag(Z) #Extract the diagonal of a square matrix
@
If for some reason you wanted to turn a matrix into vector there are few ways to do that
<<>>=
c(Z)
as.vector(Z)
@
if you have any doubts about whether something is a vector you can always check its class
<<>>=
class(x) 
class(Z)
@
\subsection{Matrix Attributes}
Just like with vectors we can use the square brackets to extract elements.  For a matrix \verb@X@, the command \verb@X[i, j]@ gives you the element from row \verb@i@, column \verb@j@.
<<>>=
X <- matrix(1:12, nrow=3)
X
X[2, 4]
@
As before we can replace individual elements
<<>>=
X[3,2]<-8
X
@
We can also extract whole rows and columns
<<>>=
X[1, ]  #First row
X[, 2]  #Second Column
X[1:2,] ##First two columns
@
Notice that when we pull out just one row or column \R\ converts it into a vector, we can use the \verb@drop@ argument to stop that
<<>>=
X[1, ,drop=FALSE]
class(X[1, ,drop=FALSE])
@
As before we can use the logical operators 
<<>>=
X[, 2] == 8 # which rows have 8 in the second column?
X[X[, 2] == 8, ]
@
For the most part \R\ treats matrices as just vectors that are written differently, this means that if we ask \R\ for things like length, mean, and standard deviation it gives it to us for all the values.  
<<>>=
length(X)
mean(X)
sd(X)
@
some things will work on directly on matrices.  Notably ways to find out the dimensionality 
<<>>=
dim(X) #dimensions of X
nrow(X) #rows of X
ncol(X) #columns of X
@
But what if we wanted means by column?  This takes us to our first introduction of the \verb@for@ loop and the \verb@apply@ function.  We will cover them in greater detail later but for now let's start with \verb@for@ loop.
<<>>=
mean.x <- rep(0, ncol(X)) #Recall that this creates a vector of 0s
#equal to the length of ncol(X)
for(i in 1:ncol(X)){
  mean.x[i] <-  mean(X[,i])  #What does this do?
}
mean.x

apply(X, 2, mean) # Same thing
@
Notice that both of the loop and \verb@apply@ do the same thing, but that apply is much easier to write.  Apply also runs much faster than a \verb@for@ loop.
So let's break down what these things do.
Before we even ran the \verb@for@ loop we created a vector in which to store the results.  
We filled the vector with missing values (\verb@NA@) but we really could have filled them with anything.  I like using missing values because it makes it easy to spot if something goes wrong.
The second thing we did was start the loop the line \verb@for(i in 1:ncol(X))@  just tells \R\ that we're going to use a variable \verb@i@ that takes the values \verb@1, 2, ..., ncol(X)@, and once \verb@i@ takes the last value in that sequence the loop is done.  The curly brackets tell \R\ the extent of the loop. \\[12pt]
The \verb@apply@ function on the other hand takes 3 arguments.  The first is a matrix, in this case that's \verb@X@. The second is a direction, 2 means that we want \R\ to apply the function over columns, 1 would mean we wanted to apply it over rows.  The last argument is a function, in example we just used means, but it could be any function, including one you write yourself once we get to writing functions.\\[12pt]
Note that one thing we can do with matrices that we can't do with vectors is name the rows and the columns.  These names are just string vectors.
<<>>=
X <- diag(2)
colnames(X)
colnames(X) <- c('left', 'right')
X
colnames(X)[2] <- 'Right'
X
row.names(X) <- c('up', 'down')
X

X[,'left']  ## We can use the names in place of numbers to index
X['up', 'left']
@

\subsection{Matrix Operations}
Matrix math in  \R\ includes  standard operations including arithmetic.
<<>>=
X <-  matrix(1:4, nrow=2)
Y <- diag(2) #Identity matrix
X + Y
X-Y
@
Note that \verb@*@ performs \emph{element-wise} multiplication.  For standard matrix multiplication use 	\verb@%*%@
<<>>=
X*Y
X %*% Y
@
If you use matrix multiplication on a vector \R\ will guess whether it is a row or column vector.  It  typically does a good job of it, but be careful.
<<>>=
c(1, 1) %*% X
X %*% c(1,1)
@
We can  transpose matrices (typically written as $X'$ or $X^T$)
<<>>=
X
t(X)
@
Matrix multiplication
<<>>=
A = matrix(1:6, nrow=2)
B = matrix(1:6, nrow=3)
A %*% B  #Matrix multiplication
@
Matrix inversion  (typically written as $X^{-1}$) is done via the \verb@solve@ command.
<<>>=
solve(X)
solve(X) %*% X
@
Note that the \verb@solve@ command is done by numerical computation, not an analytic solution, so the results are only accurate up to something like the 16th decimal place.
To illustrate we'll use \verb@rnorm@ to generate random numbers from the normal (0, 1) distribution (your answers may vary from the text due to randomness)
<<>>=
Z <- matrix(rnorm(16), nrow = 4)
solve(Z) %*% Z
@
Notice this is really close to an identity matrix, but not 	quite, we can use the \verb@round@ function to make this easier on the eyes.
<<>>=
round(solve(Z) %*%Z, digits=12)
@
So close enough for almost anything we're interested in doing.
Additional functions that may come in handy include the determinant the Cholesky decomposition
<<>>=
Y <- matrix(c(1, 0.5, 0.5, 1), nrow=2)
det(Y)
chol(Y)
t(chol(Y)) %*% chol(Y)  #make sure it worked
@
There's no command for the trace, but it's easy to figure it out with what we know
<<>>=
sum(diag(Y)) #trace
@
We can also get eigenvalues and eigenvectors
<<>>=
eigen(Y)
@
Notice that the  \verb@eigen@ output as two components designed with the \verb@$@ sign.  The \verb@$@ means that we're dealing with a list which a new type of output, to which we now turn our attention
\section{Lists}
When \R\ returns a list to us we can extract the elements of it using the dollar sign with the appropriate name.  
The names are given by the output, in the above example the names given to us are ``values'' and ``vectors''.  
If we didn't know the names we can look using the \verb@names@ command. 
The case of eigenvalues and eigenvector this would like this.
<<>>=
names(eigen(Y))
eigen(Y)$values
eigen(Y)$vectors
@
Alternatively, we can still use brackets, but with lists we have to double them up to get the specific elements.
<<>>=
eigen(Y)[[2]]
class(eigen(Y)[[2]])
@
Lists are very flexible because they are way to combine matrices of different dimensions with vectors, or to put many statistical models together in one group.
To create a list we just use the list command
<<>>=
matrixList <- list(matrix = diag(4), #Identity matrix
                   M2 = Y,
                   Eig = eigen(Y))
matrixList
@
As you can see we can even nest lists within lists.  If we wanted to extract the eigenvectors from this list we could use either the names or the square brackets.
<<>>=
matrixList$Eig$vectors
matrixList[[3]][[2]] #Same thing
@
Finally, we have two more forms of apply that we can use on just lists.  The first one we'll look at is \verb@lapply@ which is read ``L Apply'' and stands for list apply.  When we use \verb@lapply@ it performs some function that we want over the entire list.  So if we wanted to know the class of each object in a list we could do the following.
<<>>=
lapply(matrixList, class)
@
Notice that \verb@lapply@ returns  a list, this can be rather cumbersome, which is why we sometimes use \verb@sapply@ instead.  \verb@sapply@ does the same thing but returns the results in  vector form.
<<>>=
sapply(matrixList, class)
@
In most of the really useful applications of these functions we would have a list where all the elements were of the same class.  Let's say we have a bunch of matrices and want to know the column means of each one.
<<>>=
matrixList <- list(matrix1 = matrix(1:9, nrow=3), 	#3 x 3
                    matrix2 = matrix(0:5, nrow=2), 	#2 x 3
                    matrix3 = cbind(rnorm(3), 1))   	#3 x 2
matrixList
sapply(matrixList, class) # make sure they're all matrices
sapply(matrixList, dim)   # check dimensions
lapply(matrixList, apply, 2, mean)
@
Notice that in the last one we used \verb@apply@ within \verb@lapply@.  
We then just write the arguments that we would use with apply as additional arguments.  
This is something that we can generally do with functions in the apply family. For example
<<>>=
X <- matrix(c(1, NA, 1,1), nrow=2) #Row 2 has a missing value
mean(X[2, ])  #is NA
mean(X[2, ], na.rm=TRUE) #Tells R to just drop missing values
apply(X, 1, mean) #Gives us that NA
apply(X, 1, mean, na.rm=TRUE) #add option na.rm=TRUE
@

\section{Packages and Updating}
To install a  package (in this case MASS) from CRAN (99.9\% of the packages you want will be here) you just run the command 
<<eval=FALSE>>=
install.packages('MASS')
@
\noindent it may ask you to pick a mirror. I usually pick one from Pennsylvania, but it really doesn't matter which one you pick.
Once it's installed you can load it.
<<>>=
library(MASS)
@
\subsubsection{Updating \R\ and \R\ Packages}
To update \R\ there are 3 steps 
\begin{enumerate}
\item Download the new version
\item Install it
\item Uninstall the old version. 
\end{enumerate}
 In most cases that's all you'll need to do.  If there's a major \R\ upgrade (from 3.0 to 3.1) then between steps 2 and 3  you may need copy the files from your old library folder to the new one.  This isn't essential it just saves you from having to reinstall all your packages.
 
 
To update a package just run \verb@install.packages()@ again. RStudio has a button in the packages tab that says ``Check for Updates'' if you click this once every few months and select all you should be fine.
\section{Getting Help}
This is probably the most important part of the whole course.  
If you run into a problem, which will happen often, there are two things that are almost always true:  1) Someone else has had this problem and  2) Someone has solved it.
\begin{description}
\item[Finding out about a particular function:] The most common problems are related to particular functions that you want to know more about.  In these cases the best place to start looking is the \R\ help file.  These can be accessed using the ? command.  For instance if we wanted to know more about the arguments in \verb@log@, say we didn't know that it was base $e$ or we didn't know how to change it we could type
<<eval=FALSE, tidy=FALSE>>=
?log
@
Which pulls up the help file.  A typical \R\ help file consists of a few sections
\begin{description}
		\item[Description] What is the function supposed to do?
		\item[Usage] How does one typically type the command?
		\item[Arguments]  What are all the arguments and what do they do?
		\item[Details] Additional information about how the function works
		\item[Value] What does the function return?  If the function returns a list, what are the elements of that list?
		\item[See Also] Related functions that may be helpful
		\item[Example] Examples of how to use the function.
\end{description}
This is usually good enough to figure anything you want to know about a function, and running the examples at the bottom of the page can be helpful in understanding the output.
Note that if for some reason ? doesn't work you can also use type
<<eval=FALSE>>=
help('%*%') 
@
and it will do the same thing.
\item[You know what you want to do, but you don't know what function to use:] In these cases the commands ?? or \verb@help.search@ are your friends.  They do a keyword search through the help files or all your packages to find what you're looking for.  For example,
<<eval=FALSE>>=
help.search("multivariate normal")
@
Searches the help files for mentions of ridge regression.  One result that looks promising is 
\begin{verbatim}
MASS::mvrnorm  	Simulate from a Multivariate Normal Distribution
\end{verbatim}
Which means that there is a function in the MASS package called \verb@mvrnorm@
\item[If neither of those works:] Google will almost certainly find you the answer you want.  Googling ``How to do XYZ in R''  will almost always guide you to the right place.  There are few websites that deal with \R\ questions and the answers are almost always helpful.  Results from \url{www.stackoverflow.com} are usually very helpful and easy to follow, and results from the \R\ mailing list archives are also typically good.
\end{description}
Additionally, 
The library has both a hard copy and 2 (identical) electronic copies of  \emph{\R\ for  Stata Users} available.
If you have background with Stata, this may be a good reference.

\section{Exercises}
\begin{enumerate}
\item Look up the function \verb@rnorm@ using the ? function.  Read about its arguments and its related functions (\verb@pnorm@, \verb@dnorm@, etc),  we will use it in the next problem.
  \item Do the following
   \begin{enumerate}
  	      \item Create a $1,000 \times 3$ matrix, call it \verb@X@ where the first column is all 1s, the second column contains random draws from a normal with mean 1 and standard deviation 2 (hint: look at problem 1) and the last  column contains random draws from the uniform distribution [0, 1] (use ?? or google to try and find the function for this).  Use any of the methods discussed above to create the matrix.  Look up and use the function \verb@colMeans@ to  print the column means  for each column and use \verb@apply@ to print standard deviations of each column to make sure you that you did this correctly (the standard deviation for $U[0, 1]$ will be between 0.27 and 0.30)
      			\item Create a vector \verb@b@ equal to (-1, 2, 2).  Then change the second value to -2. 
						\item Use matrix multiplication to generate \verb@y@ such that $y = Xb +e $ where $e$ is a vector of length 1,000 and is distributed normal (0, 1).
					\end{enumerate}


						\item Download the following packages:
					\begin{itemize}
						\item foreign
						\item reshape2
						\item data.table
						\item plm
						\item zoo
						\item lmtest
						\item car
						\item MASS
						\item stargazer
						\item xtable
						\item ggplot2
					\end{itemize}
\end{enumerate}




<<echo=FALSE, results='hide'>>=
knitr::opts_chunk$set(comment=NA)
knitr::opts_chunk$set(prompt=TRUE, tidy=FALSE)
options(keep.blank.lines= TRUE)
opts_knit$set(root.dir="~")
@

\chapter{Control Statements and Programming}
This chapter really takes us into the meat of \R\ programming.
In particular we will cover the basics of \verb@for@ and \verb@while@ loops and if-else commands.



\section{If and Else}
When we want to use logical conditions we can use\verb@if@ and \verb@else@ as separate commands.  They have the following setup:
\begin{verbatim}
  if(LOGICAL){
    COMMAND1
    COMMAND2
  }else{
    COMMAND
  }
\end{verbatim}    
Notice the use of \verb@{}@ to contain the conditions.  
While you sometimes find code that does not use these (you don't need them for one line statements), I \emph{strongly} encourage you to always be explicit and use them as much as possible.
This makes your code less prone to breaking and much more readable to you, others, and, perhaps most importantly, your future selves.

Let's look at an example of a trivial if statement. 
<<>>=
y <- FALSE
if(y){
  cat("Hello World")
}else{
  cat("Goodbye")
}
@

We can also nest if statements.  Try  the following:  Generate a value of \texttt{test} and predict which name will be printed. 
Make sure you understand why a given name is being displayed.
<<>>=
test <- runif(1)
print(test)


if(test < 1/2){
  if(test < 1/3){
    "Mary"
  }else{
    if(test < 0.4){
      "Frank"
    }else{
      "Liz"
    }
  }
}else{
  "Bob"
}
@

Sometimes if and else can be quite cumbersome, and for special cases \R\ comes with a neat \texttt{ifelse} command.
This command takes the syntax
\begin{verbatim}
ifelse(LOGICAL,
       IF TRUE, DO THIS,
       ELSE, DO THIS)
\end{verbatim}
This can be used on vectors of logicals in ways that don't make sense for the if-else constructs we used above.
Let's try it:
<<>>=
test <- runif(10)
print(test)
ifelse(test < 1/2,
       0,
       1)
@

As with if-else constructs we can also nest them
<<>>=
print(test)

ifelse(test < 1/2,
       ifelse(test < 1/3,
              "Mary",
              ifelse(test < 0.4,
                     "Frank",
                     "Liz")),
       "Bob")
@
Were you able to predict them all correctly?  
If you did then you understand what's going on here.

\section{Loops and breaks}
Another commonly used control structure is the loop.  We can consider a couple different loops here.
The most basic, which we briefly saw above, is the \texttt{for} loop.
<<>>=
y <- 1:10
for(i in 1:10){
  y[i] <- y[i]^2
}
y
@
We can combine it with with if statements
<<>>=
y <- 1:10
for(i in 1:10){
  if(y[i] %% 2){
    print("y is odd")
  }else{
    print("y is even")
  }
}
@
Use the help functions from before to figure out what \verb@%%@ means and why we can use it to find odds and evens.

Let's say we didn't know how many times something needed done though, we just know when it's done. 
For that we can use 2 different structures.  
The first is the repeat structure:
<<>>=
repeat{
  y <- runif(1)
  if(y< .05){
    break
  }
}
y
@

We could do this OR we could do the much easier

<<>>=
#Create initial value of y that satifies the condition
y <- 1
while(y>0.05){
  y <- runif(1)
}
y
@

Typically we use for loops when we want to repeat an operation some set number of times and there is no breaking condition. 
While loops on the other hand are useful for situations where you want something to converge to within some tolerance (such as trying to maximize/minimize a function).

\section{*ply Functions}
The ply family of functions is a set of functions that are designed to make more readable (and debatably faster code).
They typically are used in place of loops because they are less cumbersome to write (once you understand them) and in some cases can give improvement in speed.
The first function we'll look at is \texttt{apply}, which is used on vectors
<<>>=
X <- replicate(3, rnorm(10))
apply(X, 2, sd) #take the standard deviation of each column
apply(X, 1, max) #max of each row
apply(X, 1, function(x){ifelse(all(x>0), return(max(x)), return(min(x)))}) 
#WTH is this?
@
If you're dealing with lists you may want to use \texttt{lapply}.
<<>>=
X <- list(A = diag(1:4),
          B = matrix(1:4, nrow=2))
lapply(X, solve)
lapply(X, t)
@
If you're dealing with lists, but you want to return a vector we have \texttt{sapply}.
<<>>=
X <- list(A = diag(1:4),
          B = matrix(1:4, nrow=2))
#Look at the difference between
sapply(X, max)
lapply(X, max)
@
Other *ply functions exist, notably, \texttt{tapply} (apply a function over a group) and \texttt{mapply}, but I don't find myself using either of those very much, so we'll leave it at that.


\section{Scripting}
Now that we're starting to get the hang of doing things in \R\ we're now at the point where we'll want to write them down so we can redo and replicate our work.
Our first script will be a program that generates some data and then provides some descriptive statistics of that data. 
To create a new script file in \R\ go to file$>$New script. In RStudio go to file$>$New$>$R Script.  In both cases we now have a blank file.  
Save this file somewhere (remember where) as ``test1.R'' and then enter the following
<<tidy=FALSE, eval=FALSE, prompt=FALSE>>=
######Heading#####
##File: test1.R
##Description: First R script

######Generate some data######
dat <- rnorm(1000)  ##Creates a vector of normal draws

######Create a function to summarize it######
summarize <- function(x){  ##This creates a function that takes one 
    																					 ##Argument, we've called x, it can be anything
																							
                                  ##Make a list with summary stats
                                  ans <- list(Mean = mean(x),
                                              StDev = sd(x),
                                              Min = min(x),
                                              Median = median(x) ,
                                              Max = max(x))  
                                  return(ans) ## Return the list we created
} ## end the function
summarize(dat) ##run the function on the data 
@
Once you have that typed, re-save the file.  We can now run the file using the \verb@source@ command.    To do this you need to have the full extension for where you saved it.
In my case this means
<<>>=
source('~/Google Drive/TA200/R Course2/test1.R', echo=TRUE)
@
Alternatively you can run individual lines  by highlight them in the file editor and press ctrl+enter.  RStudio also has a source button in built into the editor.
We can also dispense with the full extension by changing our working directory.  
<<>>=
getwd() ##Returns the current working directory
setwd('/home/casey/Google Drive/TA200/R Course2') ##Change
getwd() ##Returns the new directory
source('test1.R') ##Don't need the full extension now.
@

Now that we've sourced the file the variable \verb@dat@ and the function \verb@summarize@ are now in our working space. To see this 
<<>>=
ls()
@
Which means we can now use our \verb@summarize@ function just like any of the built in \R\ commands. For example
<<>>=
X <- cbind(rnorm(1000), 1:1000)
apply(X, 2, summarize)
@
Which we may think is too cumbersome of a result so we can collapse some of that by using the \verb@unlist@ command to collapse a list into a vector
<<>>=
lapply( apply(X, 2, summarize), unlist)
@
But we really don't want to write too many functions which is why we let other people do that and then use their packages.


% 
% \section{APPLICATION: Solving a Nonlinear System of Equations}
% Consider the following battle of the sexes with Irving and Claire.
% \begin{center}
% \begin{tikzpicture}
% \draw (0,0) rectangle (12, 6);
% \draw (6,0) -- (6, 6);
% \draw (0,3) -- (12, 3);
% \draw (-1 , 4.5)  node {$M$};
% \draw (-1 , 1.5)  node {$B$};
% \draw (9, 6.5)  node {$M$};
% \draw (3, 6.5)  node {$B$};
% \draw (-2, 3)  node {$I$};
% \draw (6, 7)  node {$C$};
% \draw (3, 4.5)  node {$2,\;\; 3$};
% \draw (3, 1.5)  node {$0,\;\;0$};
% \draw (9, 1.5)  node {$3,\;\; 2$};
% \draw (9, 4.5)  node {$0,\;\; 0 $};
% \end{tikzpicture}
% \end{center}
% 
% 
% Further suppose each player has some action-specific private information (this induces nonlinearity and makes it a little more tricky than just a system of linear equations).  
% We can think of this private information as being something like Claire discovers it's free hot dog day at the monster truck rally and so she may want to go more than is known to both players or the analyst.
% We want to find a mixed strategy equilibirum.
% 
% The expected utilities of this game are given by 
% \begin{align*}
%   \Phi_I(v_I) &= \Pr(a_C = M) U_I(a_I, a_C=M) + \Pr(a_C = B) U_I(a_I, a_C=B)\\
%   \Phi_C(v_C) &= \Pr(a_I = M) U_C(a_I=M, a_C) + \Pr(a_I = B) U_C(a_I=B, a_C).
% \end{align*}
% Let's combine those into a single $\Phi$
% \[\Phi(v) = \begin{bmatrix}
%                      \Phi_I(v_I)\\
%                      \Phi_C(v_C)
%                    \end{bmatrix}  
%                     \]
% An equilibrium of this game can be described as a vector of (4) values, $v$, such that 
% \[\Phi(v) - v = 0.\]
% This is a nonlinear system of equations which we will solve using an iterative procedure, but before we get to the procedure let's lay the ground work.
% The first thing we need is matrix of utilites from the game.
% <<>>=
% #Create the utilities 
% #U is 4 x 2 matrix where the rows are given by
% #the following action profiles (0 = B and 1 = M) 
% #a = (aI = 0, aC = 0)
% #a = (0, 1)
% #a = (1, 0)
% #a = (1, 1)
% U <- matrix(c(3,0,0,2, 
%               2,0,0,3),
%             ncol=2)
% U
% @
% <<>>=
% Next we need to write down our $\Phi$ function.  
% This requires we make an assumption on the form that the private information takes, it is not  worth going into this now.  
% Games with private information are covered more in 584 and 585, for now just take it as a condition of the exercise.
% <<>>=
% Phi <- function(v, u){
%   #Generates the output of the equilbrium 
%   #correspondance for  v and u
%   #a fixed point of this correspondence is an
%   #equilibrium
%   # v is a guess at the solution
%   
%   vI <- v[1:2]
%   vC <- v[3:4]
% 
%   #e ^ v
%   evI <- exp(vI)
%   evC <- exp(vC)
%   
%   #CCP, come from logistic private information
%   #for each player, don't worry about this right now
%   PI <- evI/(sum(evI))
%   PC <- evC/(sum(evC))
%   
%   VI <- colSums(matrix(rep(PC, 2) * u[,1], ncol=2))
%   VC <- colSums(matrix(rep(PI, each=2) * u[,2], ncol=2))
%   return(c(VI, VC) - v)
% }
% @
% The method to find the solution requires that we know the Jacobian (first derivatives) of this function at our initial guess of the solution.
% Let $x_0= \mathbf{0}$ be our initial guess, we now need to know the Jacobian of $\Phi$ at this point.
% We could, with relative ease figure this out, but not worth it for this exercise.
% Instead, I'll show you a method to approximate the Jacobian using the \texttt{numDeriv} package.
% <<eval=FALSE>>=
% install.packages("numDeriv")
% @
% <<>>=
% library(numDeriv)
% x0 <- rep(0, 4)
% D <- jacobian(Phi, #Function
%               x0, #initial guess 
%               u=u) #any additional inputs Phi needs
% D
% @
% 
% Armed with these tools we can now solve the problem.
% Broyden's method for solving non-linear equations starts with an initial guess at the solution, call this $x_0$, and does the following



\section{APPLICATION: Least Squares by Maximum Likelihood}
As you may or may not have learned by now, OLS is also the maximum likelihood estimator $\hat{\beta}$ when $y$ is distributed normally with mean $X\beta$.  
This means that solving OLS by either maximum likelihood or by minimizing the sum of squared error should give us the estimates of $\beta$. 
To satisfy us that this is the case we will use \R\ to maximize the logged likelihood function and compare it to the traditional OLS estimates.  


First, let's generate some data
<<>>=
N <- 2000
X <- cbind(1, replicate(2,rnorm(N)))
beta <- c(-1, 2, -2)
sigma2 <- 1
y <- X %*% beta + rnorm(N, 0, sqrt(sigma2))
@
Since $y$ is distributed i.i.d. normal the joint pdf of the sample is 
\begin{equation}
f(y|X, \beta, \sigma^2) = \prod_{i=1}^N \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(\frac{-(y_i- X_i\beta)^2}{2\sigma^2}\right)
\end{equation}
As you should remember from 404 with MLE we convert this joint pdf is proportional to the likelihood function so we can just switching the order of the conditionals.
To make things easy we often take the log of it.  
The likelihood function is thus:
\begin{equation}\label{linearMLE}
L(\beta, \sigma^2 | X, y)  = \sum_{i=1}^N -\frac{1}{2}\log(\sigma^2) -\frac{1}{2}\log(2\pi)  - \frac{1}{2\sigma^2} (y_i - X_i\beta)^2.
\end{equation}
As we know the $2\pi$ term is constant in $\beta$ and $\sigma^2$ so we can drop it from our likelihood routine.  
It is advisable that we reparameterize this function so that it is a function of $\beta$ and $\eta = \log(\sigma^2)$.
The reason why we reparameterize the model was so that we can let \R\ take guesses at $\eta \in \mathbb{R}$ rather than $\sigma^2 \in \mathbb{R}_+$.  
Numerical optimizers are much easier to work with if you can find a way to let them take guesses that are not constrained.
Our new likelihood is now
\begin{equation}\label{linearMLE2}
L^*(\beta, \eta | X, y)  = \sum_{i=1}^N -\frac{1}{2}\eta   - \frac{1}{2\exp(\eta)} (y_i - X_i\beta)^2.
\end{equation}
Now when our optimizer takes guesses at $\eta$ it can guess any real number and our estimate for  $\sigma^2$ is $\widehat{\sigma^2} = \exp(\hat{\eta})$.
Let $\theta = (\beta, \eta)$ we can now create the likelihood function.  
<<>>=
NormalMLE <- function(theta, X, y){
  eta <- theta[length(theta)] #extract eta from parameter vector
  beta <- theta[-length(theta)] #beta coefficients
  
  Lik <- - 1/2 * eta - 1/(2*exp(eta)) * (y - X%*%beta)^2 #L* from above
  Lik <- -sum(Lik)
  return(Lik)
}
@
Note that we waited until the end to sum up the observations.  Also note that we took the negative of the sum.  This is because most (but not all) numerical optimizers look for a minimum rather than a maximum, minimizing the negative likelihood is the same as maximizing the likelihood.

It is advisable in cases of numerical optimization that we also include first derivative information on the parameters.
The gradient takes the form
\begin{align}\label{gradMLE}
\frac{\partial L}{\partial \beta} &= \sum_{i=1}^N \frac{X_i(y_i -X_i\beta)}{\exp(\eta)} \nonumber\\
\frac{\partial L}{\partial \eta} &= \sum_{i=1}^N\left( \frac{(y_i -X_i\beta)^2}{2\exp(\eta)} -\frac{1}{2}\right)
\end{align}
and returns a vector of length equal to to the length of $\theta$. 
The actual programming will be left as an exercise for the reader but here's what the output should look like
<<echo=FALSE>>=
grNormalMLE <- function(theta, X,y){
  eta <- theta[length(theta)] #extract eta from parameter vector
  beta <- theta[-length(theta)] #beta coefficients
  
  dBeta <- (X * as.numeric(y-X%*%beta))/exp(eta)
  dEta <- (y- X%*%beta)^2 / (2*exp(eta)) - 1/2
  D <- colSums(cbind(dBeta, dEta))
  return(-D)
}
@
<<>>=
#Should look something like this, but your results will 
#differ because X and y are generated
grNormalMLE(rep(0,4), X=X, y=y)
@
We don't really need the gradient to optimize the function it's just useful for improving accuracy, speed, and reliability.
However, in a lot of problems it's unnecessary.  
To actually optimize the function we will use the \texttt{optim} function.
<<>>=
##optim is a nonlinear optimizer that takes the following inputs
#par = starting values, in our case draws from the uniform. 
#      These correspond to theta above
#fn = function to optimize
#method = Method to use for optimization BFGS is a quasi-Newton method 
#         that works really well on most problems
#gr = gradient (first derivatives)
#X, y are the extra arguements that we included in the  NormalMLE 
#     and grNormalMLE functions.
optim(par=runif(4), fn=NormalMLE, method="BFGS", X=X, y=y)
optim(par=runif(4), fn=NormalMLE, method="BFGS", X=X, y=y, gr=grNormalMLE)
@
This is nice what if we wanted to standard errors though.  
\texttt{optim} doesn't have an option for that directly but it can return the Hessian.
Do you remember how to get standard errors from the Hessian (404 material)?
<<>>=
mod1 <- optim(par=runif(4), fn=NormalMLE, method="BFGS", X=X, y=y, 
                hessian=TRUE)
mod2 <- optim(par=runif(4), fn=NormalMLE, method="BFGS", X=X, y=y, 
              gr=grNormalMLE, hessian=TRUE)

vcov1 <- solve(mod1$hessian)
vcov2 <- solve(mod2$hessian)
sqrt(diag(vcov1))
sqrt(diag(vcov2))
@
Comparing these results to OLS estimates is left as an exercise to the reader.
\section{APPLICATION: Monte Carlo on Omitted Variable Bias}
This application will use a Monte Carlo experiment to explore the effect of omitted variable bias in a linear model.
A Monte Carlo experiment is a simulation experiment wherein we set the true values of data to see how models perform in particular circumstances.
In this application we will be seeing how the linear model performs in cases where a relevant explanatory variable is not included.
For each iteration of the Monte Carlo we will do the following:
  \begin{enumerate}
  \item Generate data using the following data generating process:
    Draw $X_1$ and $X_2$ from the multivariate normal with mean 0, correlation $\rho$, and $\sigma^2_{1} =\sigma^2_{2}=1$. Use $\rho = (-0.5, 0, 0.5)$.
    $$y = 1 -2(X_1) +2(X_2)+\varepsilon$$
    Where $\varepsilon \sim N(0,1)$.  Create 2,000 observations in each sample.
  \item Estimate $\hat{\beta}$ using OLS of only $y$ on $X_1$.
  \item Calculate the bias by subtracting the true values of $\beta$, $(1, -2)$ from the estimated values that you get from \verb@lm@
  \item Store this bias
  \item Repeat 1,000 times
  \item Create a list of length 3 (one for each value of $\rho$).  Within that list create a $2\times 3$ matrix where row 1 is the mean and 95\% Confidence Interval of the bias the  of $\hat{\beta}_0$, and row 2 is the same for the bias of $\hat{\beta}_1$.
\end{enumerate}
So what does this look like?
<<>>=
library(MASS) #for the multivariate normal
N <- 2000 #Sample size
rho <- c(-0.5, 0, 0.5) #Values of rho
beta <- c(1, -2, 2) #True betas
MCresults <- list() #empty list

for(r in 1:3){ #loop over values of rho
  results <- matrix(0, nrow=1000, ncol=2)
  for(i in 1:1000){
    Sigma <- matrix(c(1, rho[r], rho[r],1), nrow=2)
    X <- mvrnorm(N, c(0,0), Sigma)
    y <- cbind(1, X) %*% beta + rnorm(N)
    X1 <- cbind(1, X[,1])
    bhat <- solve(t(X1)%*%X1)%*%t(X1)%*%y
    bias <- beta[-3] - bhat #what's the -3 do?
    results[i,] <- bias
  }
  biasOut <- cbind(colMeans(results),
                   t(apply(results, 2, quantile,  #WTH is this?
                           c(0.025, 0.97)))) #and this?
  MCresults[[r]] <- biasOut
}
@

What we can see from this particular Monte Carlo is that the constant term remains roughly unbiased, but there can be noticeable bias on the coefficient on $X_1$.
More importantly the size and direction of the bias varies depending on how the omitted variable is related to the variable included.
You'll cover this problem in more detail in 405.
\section{Exercises}
\begin{enumerate}
\item Create a function that takes a vector of numbers and returns the maximum.            
\item In this exercise you will use Gibbs sampling to estimate a Bayesian linear regression model.n
    First some background information We know so far 
    \[y|X, \beta, \sigma^2 \sim N(X\beta, \sigma^2 I). \]
    Since this is Bayesian we need to assume a prior distribution on $\beta$ and $\sigma^2$.
    The standard priors are from a diffuse uniform.
    
    
    As you may recall we need to identify the conditionals of our parameters.
    I'll spare you the details the distributions we want work out to be (as you might guess)
    \begin{align*}
    \beta| \sigma^2, y, X &\sim N\left( (X'X)^{-1}X'y, \sigma^2(X'X)^{-1}\right)\\
    \sigma^2|\beta, y, X &\sim \text{Inv-}\chi^2\left(N-k, \frac{1}{N-k}(\hat{e}'\hat{e})\right),
    \end{align*}
    where $\hat{e} = y-X[(X'X)^{-1}X'y].$
    
    We will do this in steps:
    \begin{enumerate}
      \item Generate data where $y = X\beta + \varepsilon$, where $\varepsilon \sim N(0,\sigma^2)$. 
      Set $\beta = (-1, 2, -2)$ and $\sigma^2= 4$.  Have $X$ be a matrix of a constant term and two random normal variables.  Set $N=2,000$.
      \item Initialize a matrix of dimension $10,000 \times 4$, fill it with 0s.
      \item Draw initial values for $\hat{\beta}$ from a uniform from $-100,000$ to $100,000$
      \item Construct a for loop that runs for 10,000 iterations.  Each iteration should 
          \begin{itemize}
            \item Take the last draw of $\beta$ and set it as the current value of $\beta$.
            \item Draw $\sigma^2_i$ from the inverse $\chi^2$ (you'll need \texttt{geoR::rinvchisq}).
            \item Draw $\beta_i$ from the multivariate normal (you'll need \texttt{MASS::mvrnorm}).
            \item Store the vector $(\beta_i, \sigma^2_i)$ in the matrix you previously initialized as row $i$.
          \end{itemize}
    \item When this is done running take the resulting matrix and discard the first 5,000 rows.
    \item Take the mean of each column, it should be about equal to the true value of $(\beta, \sigma^2)$.
    \end{enumerate}
    
    
\item  Recall that the OLS estimator is 
						$$\hat{b} = (X' X ) ^{-1} X' y$$
						with variance estimator 
						$$s (X'X)^{-1}$$
						where $s= \dfrac{\hat{e}'\hat{e}}{(N-k)}$,  and $\hat{e} = y - X\hat{b}$.  
            Now do the following
            \begin{enumerate}
              \item			Your task is to create a function that takes 2 inputs \verb@X@ and \verb@y@ (\verb@function(X, y)@) as inputs and returns a list containing  the OLS estimates, the variance matrix and the standard errors (Recall the standard errors are equal to the square root of the diagonal of the variance matrix).
     Note, you may have to use \verb@as.vector@ on \verb@s@ to avoid an error.
              \item test this function on the data you generated in exercise 2.
						  \item Look up the function \verb@pt@ and edit your function from the last part to conduct a $t$ test  to see if each coefficient is statistically significant from 0.   
            Return a matrix with the following four columns: Estimate, Standard Error, $t$ statistic, $p$ value, name them ``Est'', ``SE'', ``t'', and ``p'', respectively.
						Recall that the $t$ statistic in this case is 
						$$t = \frac{(\hat{b}-0)}{S.E.}$$
						(Hint: In order to get the right $p$ value use the absolute value of the $t$ statistic, the upper tail of the $t$ distribution, and multiply your final answer by 2)    
            \end{enumerate}
\item Code the gradient listed in equation \ref{gradMLE}.
\end{enumerate}


<<echo=FALSE, results='hide'>>=
knitr::opts_chunk$set(comment=NA)
knitr::opts_chunk$set(prompt=TRUE, tidy=FALSE)
options(keep.blank.lines= TRUE)
@

\chapter{Data Frames and data tables}
Today we'll be focusing on one particular type of object, the data frame.
Data frames in \R\ are used for data manipulation and data analysis because they offer a few advantages over the standard matrix, the advantages that they offer are:
\begin{itemize}
  \item Each column in a data frame can be of a different class (numeric, character, factor).  All the columns in a matrix must be the same class (numeric, character).
	\item Data frames can be merged together, the \verb@merge@ command doesn't work on matrices
	\item Most canned regression models are designed to work with data frames rather than matrices
	\item It's easier to extract individual variables out of a data frame
\end{itemize}
Because data frames are pretty essential to most applications of \R\ we'll be doing a lot of specific applications.  

Before we continue please make sure you have the most recent versions of the following packages installed:
\begin{enumerate}
	\item readstata13
	\item tidyverse
	\item data.table
	\item dplyr
\end{enumerate}

\section{Reading data}
One advantage of \R\ over other statistical packages is that it has the ability to read many different kinds of data.  
The two standard read commands are for tab and comma separated data and they are \verb@read.table@ and \verb@read.csv@, respectively.  
It's easy to save excel files into comma separated data (.csv), and I would recommend this over using tools explicitly designed for excel files.
For many purposes the combination of \verb@read.csv@ will get you where you want to go however, there are lots of times when the data can only be obtained in Stata (.dta) format.  
In these cases we can use the \verb@foreign@ package which can read files produced by Stata (up to version 13), SAS, SPSS, S+, minitab,  .dbf files (GIS data is often in .dbf form) and other data formats.


For newer Stata files you can use either \verb@readstata13@ or \verb@haven@.
Haven is part of the "tidyverse" which is a set of packages that form an easy and increasingly popular way to do things in \R.
I also like \verb@data.tables@ as a faster alternative to the tidyverse. 
For each thing today we'll talk about the base and tidy way to do things. I will eventually include the data table ways as an appendix to this chapter.


The tidyverse has several packages for reading data \verb@haven@ for dta files, \verb@readr@ for most text data (csv, txt, tab),  and \verb@readxl@ package for excel-style files (xls and xlsx).
To see these in action, we'll read in the data files that I sent you this morning.
In addition to reading data from outside sources, many \R\ packages (including the base packages) come prepackaged with datasets which can be accessed using the \verb@data@ function
<<>>=
# Tidy packages
library(readr)
library(dplyr)

# for stata files (i like it better than haven)
library(readstata13)
@
Note that loading the Tidyverse packages tells us what is included.
<<>>=
getwd()
dir()

##Notice that I use relative pathes below. You could use the setwd()
##command that we learned last time to change the working directory to 
##directory that contains the datasets folder.


##ordinary csv
NMC <- read_csv('Datasets/NMC_Supplement_v4_0.csv')

##stata dataset
FL2003 <- read.dta13('Datasets/FearonLaitin_CivilWar2003.dta') 
# A warning. Let's do what it says
FL2003 <- read.dta13('Datasets/FearonLaitin_CivilWar2003.dta',
                     nonint.factors=TRUE)

class(NMC)
class(FL2003)
@
Notice that among the classes here is `data.frame` which is what we're into.
Also note, that `read_csv` includes some extra info about the file that's loaded. 
We can suppress that with the option `show_col_types=FALSE`.
Now we've read in the data we can take a look at it.

\section{Commands to use on Data}
	\subsection{Looking at the Variables}
	Once we've read in the data we may wish to look at it.  This can be accomplished using the \verb@View@ command.  
	This command opens up a new window where we can see the data just like we would using the browse command in Stata, there is also the command \verb@fix@ which is the equivalent of the edit command in Stata.
<<eval=FALSE>>=
View(NMC)
fix(NMC)
@
There is also an easy way to just look at the first few observations of a data.frame.  This is helpful just to see what the variables look like without actually looking at the whole dataset.  This can be done using the \verb@head@ command.  Additionally, the command \verb@summary@ can be used to get a summary of each column in the data frame; we can also look at just the variable names using the command \verb@colnames@
<<>>=
head(FL2003) #Top  6
tail(FL2003) #Last 6
summary(FL2003[, 1:10]) ##Truncated the first 10 columns to save space
colnames(FL2003) 
@ 
It's worth noting at this point that all of commands just mentioned work on matrices, and everything but \verb@colnames@ works on ordinary vectors.

\subsection{Individual Variables}
\R\ treats data frames like a special version of a list.  
This means that to access individual elements we use the dollar sign. 
For example if we want just the summary of the \verb@pop@ variables in Fearon and Laitin we would type.
<<>>=
summary(FL2003$pop)
@
We could also use numbers to index like last week
<<>>=
summary(FL2003[,18]) ##But isn't the dollar sign easier?
@
Extracted variables are just vectors and so we can treat them as such
<<>>=
##Doing vector stuff with variables
FL2003$pop[1:10]
head(log(FL2003$pop)) 
@


\subsection{Creating Subsets}
We can also use index to create subsets of data frames, for instance if we just wanted the COW codes and years we could do any of the following to create that subset.
<<>>=
##These all do the same thing
temp.dat <- FL2003[, c('ccode', 'year')]
head(temp.dat)

temp.dat <- FL2003[, c(53, 2)]
head(temp.dat)


temp.dat <- cbind.data.frame(FL2003$ccode, FL2003$year) 
head(temp.dat)##notice this way messes up the column names

temp.dat <- with(FL2003, cbind.data.frame(ccode, year))
head(temp.dat)

# base R preferred way
temp.dat <- subset(FL2003, select=c('ccode', 'year'))
head(temp.dat)

# OR the tidy way
temp.dat <- FL2003 %>% 
  select(ccode, year)
head(temp.dat)
@
In general, \verb@subset@ or \verb@select@ is probably the better choice
Note that we have introduced the \verb@%>%@ operator.  
The  \verb@%>%@ operator is called the pipe and it links commands together in easier to read ways.
So instead of $f(g(h(x)))$ we can have $x \to h() \to g() \to f()$. 
In other words, we can write them in the order we want rather than from in to out.  


We can also subset based on rows
<<>>=
##These all do the same thing
temp.dat <- FL2003[FL2003$ccode ==2, ] ##Extract USA 
head(temp.dat)

# base R
temp.dat <- subset(FL2003, ccode==2)
head(temp.dat)

#tidy
temp.dat <- FL2003 %>% 
  filter(ccode==2)
head(temp.dat)
@
Notice that the second time we didn't use the \verb@select@ argument.  
If we pull up the help page on \verb@subset@ (\verb@?subset@) we can see that in the second chunk we actually used the ``subset'' argument. 
The subset argument takes a logical expression (in this case \verb@ccode==2@) for selecting rows that we want.  The select argument takes column names for the columns that we want.  
We can use them to together
<<>>=
temp.dat <- subset(FL2003, ccode==2, select=c('year', 'polity2'))
head(temp.dat)
dim(temp.dat)

#tidy
temp.dat <- FL2003 %>%
  filter(ccode==2) %>%
  select(year, polity2)
head(temp.dat)
dim(temp.dat)
@
% \begin{note}
% I include the expression \verb@!is.na(FL2003$ccode)@ as a precaution.  
% If we don't include this and there are NAs (Missing Values) in one of the variables we're indexing on, then \R\ has a problem processing it and we end up with lots of extra rows that are all NAs.
% \end{note}

\subsection{Classes}
One thing you might have noticed when we ran \verb@summary()@ on the Fearon and Laitin data is that not all variables looked the same.  For instance if we run
<<>>=
temp.df <- FL2003 %>%
   select(ccode, cname, region) 
summary(temp.df)
lapply(temp.df, class) ##lapply because it's really a type of list
@
We can see that we have a numeric variable, a character variable, and a factor variable.
In general, \R\ assigns these classes when we read the data, and most of the time it gets it right.
Numeric and integer variables are variables that are all numbers.  
These are ordinary variables, they can be either continuous (population) or discrete (year) and \R\ won't notice the difference.
Everything we covered with numeric vectors last time works on these.
Character variables are just strings.  
There's not too much special we can or would want to do with these.
Factors, however, are an interesting construct.

\subsubsection{More on Factors}
Factors are how \R\ deals with categorical variables. 
In the Fearon and Laitin example region is stored as a factor.  
Running \verb@summary@ on a factor variable returns a table with a count of each category.
<<>>=
summary(FL2003$region)
head(FL2003$region) ##tacks on info about the levels

levels(FL2003$region) ##Just want to know the levels
nlevels(FL2003$region) ##Just want to the number of levels
@
Where the first level is the reference level
Factors can be troublesome when manipulating data.  
To get around this you may sometimes want to convert factors to characters when doing any manipulation.
For example if we want to subset the data to remove one level from a factor \R\ will do that but it won't drop that as a level, which can mess things up.

<<>>=
temp.df <- FL2003 %>% 
  filter(region=='western democracies and japan')
summary(temp.df$region) ##others still listed
@
We can tell \R\ to convert all factors to characters when we read in the data.  
Likewise \R\ sometimes messes up and creates factors where we don't want them (it will sometimes read a numeric or a character in as a factor).
We can easily change things between classes
The only transformation we need to be careful with is 
<<>>=
FL2003 %>% 
  select(pop) %>%
  head()

FL2003 %>% ##change from numeric to character
  mutate(pop=as.character(pop)) %>%
  select(pop) %>% 
  head()


FL2003 %>% ##change from numeric to character to factor
  mutate(pop=as.character(pop),
         pop=as.factor(pop)) %>%
  select(pop) %>% 
  head()

FL2003 %>% ##change from numeric to character to factor to numeric
  mutate(pop=as.character(pop),
         pop=as.factor(pop),
         pop=as.numeric(pop)) %>%
  select(pop) %>% 
  head() #WHOOPS


##R numbers them by the level their in, 
##so the first level (222) is converted to 1

FL2003 %>% ##change from numeric to character to factor to numeric
  mutate(pop=as.character(pop),
         pop=as.factor(pop),
         pop=as.character(pop),
         pop=as.numeric(pop)) %>%
  select(pop) %>% 
  head() #What a relief

@
Useful transitions:
\begin{table}[H]
\centering
\caption{Useful functions for Converting Objects}
\begin{tabular}{rr}
Function                  &   Use \\ \hline
\verb@as.numeric@         &   Change a factor or character vector into numbers\\
\verb@as.character@       &   Change a numeric or factor vector into a character string\\
\verb@as.Date@            &   Change a character vector of dates in a Date object\\
\verb@as.factor@          &   Change a character or numeric vector in factor\\
\verb@as.matrix@          &   Change a vector or data frame into a matrix\\
\verb@as.data.frame@      &   Change a matrix into a data frame\\ \hline
\end{tabular}
\end{table}


Before moving on, we can say a few more things about factors
<<>>=
x <- 1:5
factor(x)
factor(x, levels = 1:10) ##Add extra levels that aren't in x
factor(x, labels = c('blue', 
                     'red', 
                     'green', 
                     'yellow', 
                     'pink')) ##Relabel x
@
We'll do more with factors when we do analysis in the next session. They become more useful then.

%%%Peter's table%%%%

\section{Merging Data}
The \verb@merge@ function in \R\ is important enough to merit its own section, although it's relatively easy to do.
The function takes two data.frames and joins them together based on one or more columns that the user supplies.  Let's start with a simple example.
<<>>=
#####Create two data frames######
temp.df <- data.frame(ccode= 1:5,
                      Var1= rnorm(5))

temp.df

temp.df2 <- data.frame(ccode= 1:5,
                       Var2 = runif(5))
temp.df2

temp.df3 <- merge(temp.df, 
                  temp.df2,
                  by='ccode') ##The variable we want to merge on

temp.df3 ##Ta Da
@
A slightly more complicated example might be
<<>>=
temp.df <- data.frame(cow.code= 1:5,
                      Var1= rnorm(5))

temp.df

temp.df2 <- data.frame(ccode= 1:5,
                       Var2 = runif(5))
temp.df2

####We want to merge of country codes, but they have different names###
####Not to fear
temp.df3 <- merge(temp.df, 
                  temp.df2,
                  by.x='cow.code', ##.x refers to the 1st data.frame
                  by.y='ccode')   ##.y refers to the 2nd

temp.df3 ##Ta Da
@
An even more complex example
<<>>=
###Contains different countries
temp.df <- data.frame(cow.code= 1:10,
                      Var1= rnorm(10))

temp.df

temp.df2 <- data.frame(ccode= c(1:5, 11:15),
                       Var2 = runif(5))
temp.df2

####We want to merge of country codes, but they have different countries###
temp.df3 <- merge(temp.df, 
                  temp.df2,
                  by.x='cow.code',
                  by.y='ccode')  

temp.df3 ##Note it only contains overlapping countries

#####All the countries from just the first data.frame####
merge(temp.df, 
      temp.df2,
      by.x='cow.code',
      by.y='ccode',
      all.x=TRUE)

###Same for the 2nd
merge(temp.df, 
      temp.df2,
      by.x='cow.code',
      by.y='ccode',
      all.y=TRUE)

###All from both
merge(temp.df, 
      temp.df2,
      by.x='cow.code',
      by.y='ccode',
      all=TRUE)

@
We can turn to the real data to show that we can match on more than one variable.
<<>>=
mergedData <- merge(FL2003,
                    NMC,
                    by=c('ccode', 'year'), ##Variables to match on
                    all.x=TRUE) ##Keep all the values from FL2003
@
More information on \verb@merge@ can be found in its help file. It's very flexible and very straight forward.

\section{Reshaping Data}
Sometimes we get data that need to be reshaped.  
Some common examples are Freedom House or World Bank data which typically comes in a wide format. 
The tidy form of this task involves pivot functions

<<>>=
library(tidyr) # tidy pivoting functions

##Freedom House data on Freedom of the Press
pressData <- read_csv('Datasets/Press_FH.csv') 

##It has  a column for country names and then a bunch of years
##We want to reshape it into a country year format
colnames(pressData) 


pressData <- pressData %>%
  pivot_longer(cols = !country, #colnames to swing around (everything but country)
               names_to ='year', ##What to call column that is 
               # now the old column names
               names_prefix = "X", #removeing prefix
               values_to = 'press'  ) %>% ##What to call column with the data

  mutate(year  = as.numeric(year))

head(pressData)
##We can reconvert it using the 'pivot_wider' function, but rarely useful
@

\section{Generating New Variables}
You saw above that we introduce `mutate` as a way to change variables. We will spend more time with that now.
Often we want to either change or create new variables that we want to add to our data frame.  
In most cases this is pretty easy.
For instance if we wanted might notice that the Fearon and Laitin data doesn't contain logged GDP per capita.  To create that we could do the following
<<>>=
###Creates and attaches the new variable to the data frame
#base way
FL2003$lgdp <- log(FL2003$gdpen)

#tidy 
FL2003 <- FL2003 %>%
  mutate(lgdp = log(gdpen))
@
\subsection{Removing Variables}
Removing variables is also straight forward. We can do it one at a time or with the subset command.
<<>>=
#tidy 
FL2003 <- FL2003 %>% 
  mutate(random = rnorm(nrow(FL2003)),
         random = NULL) ##Remove this variable
head(FL2003)
@
We've barely scratched the surface here, the tidyverse also includes ways to select columns by location, what they start with, what they end with, or just contain. Check the help file on `starts_with` for more info here.

We'll now look at some applications  of common data tasks.
\subsection{APPLICATION: Generating Dummies}
Generating dummy variables is a common task and there lots of ways to do it.  First let's just look at a making a dummy for democracy
<<>>=
#We can do it directly
FL2003 <- FL2003 %>% 
  mutate(demDummy = as.numeric(polity2 >= 7))
summary(FL2003$demDummy) 



## or with if and else
FL2003 <- FL2003 %>% 
  mutate(demDummy = ifelse(polity2 >= 7, 1,0))
summary(FL2003$demDummy) 
@
The \verb@ifelse@ command rolls an if-then-else statement into one command.  
It's nice for more complicated situations where we maybe want to apply different functions depending on the logical.\\[12pt]
We can also generate a whole set of dummies from a single variable (i.e. country or year dummies)
<<>>=
#base R
cDummies <- model.matrix(~factor(cname) - 1, data=FL2003)
FullData <- cbind(FL2003, cDummies) 
colnames(FullData)[c(1:10, 55:80)] ##Take a look

# Tidy solution: hack pivot wider
FullData <- FL2003 %>%
  mutate(const=1) %>%
  pivot_wider(names_from = cname, values_from = const, 
              names_prefix = "cname_", values_fill = 0)
colnames(FullData)[c(1:10, 55:80)] ##Take a look
@
The command \verb@model.matrix@ uses what's called a formula in \R.
We'll go in to formulas more extensively when we start estimating models, but for now I'll note that the above command is an internal function that  \R\ uses when it's getting ready to create a matrix of variables whenever it runs a regression.
We just borrowed it for making dummies.
Formulas for regression take the form \verb@Y ~ X@.  
So the above formula has no dependent variable, country dummies as the only independent variables, and no constant (the -1 term).
Including no constant means that it generated a dummy for all the countries (with a constant it would drop one).


\subsection{APPLICATION: Generating a Lagged DV}
Another common task is creating lagged variables.  
There's not a great base \R\ solution here, but tidy does great
<<cache=TRUE>>=
FL2003 <- FL2003 %>% 
   arrange(ccode, year) %>%
  group_by(ccode) %>%
  mutate(LagOnset=lag(onset))

# Did it work?
FL2003 %>% 
  filter(ccode==200 & year > 1965) %>%
   select(cname, year, onset, LagOnset) %>% 
  head() 
@

% 
% We also introduced a new way to build a data frame. 
% In the second example we used the command \verb@data.frame()@ to create an empty data frame and then used \verb@rbind@ to build it up from nothing.
% Generally, this approach is not recommend because it uses a lot of memory building these data frames and putting them together, but if you can't think of another way to do something then it'll get the job done.


% Finally, we introduced \verb@tapply@ another member of the \verb@apply@ family.
% \verb@tapply@ takes three basic arguments which are similar to \verb@apply@. 
% The first argument is the variable we want to do something to, in the first usage this was \verb@year@.  
% The second argument asks us what the group variable is, in both usage we told it to go by country (\verb@ccode@).
% The third argument is the function to apply to the variable.
% In this case we used a function that defined were we first apply \verb@as.zoo@ and then \verb@lag@ to the input.  
% The last example uses the plm package to tell \R\ we have a panel and then use it the \verb@lag@ function from plm, \R\ knows which version of \verb@lag@ to use based on the class of the object.
% Of all these examples \verb@tapply@ runs the fastest.

\section{APPLICATION: Aggregating Data}
As a final application, there may be a situation where you have data that you want to aggregate.  

<<>>=
###Generate some data####
newDat <- data.frame(ccode=rep(1:5, each=10),
                     year = rep(1:5, 10),
                     Var1 = rnorm(50))
###This is a data frame with 5 countries and five years
##But each country-year has two observations.  
##We want to aggregate by sum

library(data.table)
newDat <- data.table(newDat)
newDat[, AggVar := sum(Var1), ##New variable with definition
       by=c("ccode", "year")] ##aggregate over these 
@
Note that this follows a completely new syntax than what we're used to using.  
Data tables are very easy to work with and usually they work very fast.
We could spend a whole day on data tables, their syntax, and how to use them.  
However, I feel that's beyond the scope of this course.  
Inquiring minds may wish to check out \href{http://www.robertjcarroll.com/bigdata.html}{this special presentation} by a former Star Lab fellow on the matter.
\section{Writing Data}
Once we have our data all set we may want to save it.  All of the read functions we used have write equivalents.
<<>>=
write.csv(NMC, 'NMC.csv')
write.dta(NMC, 'NMC.dta')
save(list=c('FL2003',
            'NMC'),
     file='DataSets.rdata')
save.image('DataFrames.Rdata')
@
The write functions create individual files that can be opened by excel or stata, whereas the .Rdata files are specific to R and can contain any number of objects.  
\verb@save@ lets you save specific objects, and \verb@save.image@ saves your entire workspace.
<<>>=
ls() #Everything
rm(list=ls())
ls() #Nothing
load('DataFrames.Rdata')
ls() #It's all back
@
\section{Exercises}
This was probably the hardest section to create notes for because when it comes to manipulating data there are so many different ways to do the same thing, and there are so many possible tasks that could come up.
The only way to really get the hang of data manipulation in \R\ is to have a project where you do everything in \R.  
\begin{enumerate}
\item This exercise focuses on read data and manipulating it.  In order to get the most out of it make sure that you're starting with an empty work space and no extra packages loaded.  Try to load only the packages you need.
\begin{enumerate}
      \item Read in the FH\_press data and the Fearon and Laitin data.
      \item Using the \verb@melt@ command transform the FH data into country-year format.
      \item Load the stringr package and use the ? function to look at the help file for the function \verb@str_replace@.  We want to drop the \verb@X@ that is in front of all the years in the melted press data. (use \verb@head@ to look at the melted data if you don't know what I mean.) (HINT: Replace the \verb'X' with \verb@''@).  Overwrite the year variable with the output from \verb@str_replace@.
      \item Source the file ``stringr\_to\_cow.R" in the datasets folder.  This reads in a function that I made to convert from country names to COW codes.  The syntax it takes is the following.
<<eval=FALSE>>=
NEWDATA <- stringr.to.cow(OLDDATA$COUNTRYNAMEVAR, ##Country names
                                                  ##String format
                          OLDDATA$YEARVAR,  ##year var
                          OLDDATA) ## name of data.frame

##So if your data is called pressNew and you want to overwrite it   
pressNew <- stringr.to.cow(pressNew$country, 
                           pressNew$year,
                           pressNew)
@
      Note that the data has places that aren't really countries (like Hong Kong), for these values it returns a 0. Drop all the rows where ccode == 0.  Also drop the country name variable and all the values of press equal to \verb@'N/A'@.
      \item Once you have that merge it with the Fearon and Laitin data by ccode and year.  Make sure that the number of rows in the merged data matches the number of rows in the original Fearon and Laitin data.
      \item Generate country and year dummies and cbind them to your merged data.
\end{enumerate}
\end{enumerate}

<<echo=FALSE, results='hide'>>=
knitr::opts_chunk$set(comment=NA, fig.width=6, fig.height=4, fig.align='center')
knitr::opts_chunk$set(prompt=TRUE, tidy=FALSE)
options(keep.blank.lines= TRUE)
@


\chapter{Plotting}
Today we'll be looking at graphics in \R. \R\ has three major plotting systems: \texttt{base}, \texttt{lattice}, and \texttt{ggplot}. 
All three do the same things and so we really only need to learn one. 
Most of the grad students and other \R\ enthusiasts like to use \texttt{ggplot} because it produces nice looking plots, it's more consistent in syntax across difference type of plots than base graphics, and the options make more sense to me.
To use \texttt{ggplot} we need to use the \texttt{ggplot2} library. 
We'll also use the \texttt{gridExtra} library to arrange multiple plots into a single figure.

\section{Basic Plots}
Despite the good things about ggplot sometimes is nice to do some some basic, exploratory
plots with base graphics.
<<>>=
x <- -10:10
y <- x^2
plot(y~x)
dat <- data.frame(x=x,
                  y=y)
with(dat, plot(y~x))
@
You can spice these up by using functions like \texttt{lines} or \texttt{points}.
To get a fast histogram we can do this:
<<>>=
x <- rnorm(1000)
hist(x, freq=FALSE) #To get a true histogram

@
\section{Scatterplots and Layers}
We'll start with basic plots, using data from Peter's website
<<>>=
library(ggplot2)

FE2013 <- read.csv("http://www.peterhaschke.com/Teaching/R-Course/FE2013.csv")
colnames(FE2013) ##Take a look at the variables
@

\verb@ggplot@ relies on layers which are connected using the `+' sign.  The first layer is created using the \verb@ggplot@ command on a data.frame.
\begin{note}
\verb@ggplot@ ONLY works on data.frame objects.
\end{note}
<<eval=FALSE>>=
plot1 <- ggplot(FE2013)
plot1 ##It's blank
@
To create the scatterplot we need to add that layer to the plot
<<>>=
plot1 <- ggplot(FE2013) +  ##Initial layer
            geom_point(aes(x = FEhighway, y=FEcity))

###geom_point is used to specify that we want a scatterplot
###aes is used to specify the variables used in the plot

plot1
@

It's pretty straight forward to make changes to plot once it's created.  In this case say we wanted to add a title and change the axis labels.\\[30pt]

<<>>=
plot1 <- plot1 + ##Take plot1 and add
          xlab('Miles per Gallon: Highway')+
          ylab('Miles per Gallon: City')+
          ggtitle('Fuel Economy')

plot1
@
We can add color to the plot by including a factor variable. In this case, let's color the observations by number of cylinders.
<<>>=
FE2013$Cylinder <- factor(FE2013$Cylinder) ##Need to convert to a factor

plot1 <- ggplot(FE2013) + ##Since we changed the data we need to start over
          geom_point(aes(x=FEhighway, y  = FEcity, color= Cylinder))+
          xlab('Miles per Gallon: Highway')+
          ylab('Miles per Gallon: City')+
          ggtitle('Fuel Economy')
plot1
@
<<>>=
##Can use different shapes in place of color
plot1 <- plot1 + 
          geom_point(aes(x=FEhighway, y  = FEcity, shape= Cylinder))

plot1 
@
<<>>=
##Or sizes
plot1 <- plot1 + 
          geom_point(aes(x=FEhighway, y  = FEcity, size= Cylinder))
plot1
@
Alternatively we can adjust the color, shape, and size all the points if we do it within \verb@geom_point@ and outside \verb@aes@
<<>>=
plot1 <- ggplot(FE2013) + 
          geom_point(aes(x=FEhighway, y  = FEcity), color="blue")+
          xlab('Miles per Gallon: Highway')+
          ylab('Miles per Gallon: City')+
          ggtitle('Fuel Economy')
plot1
@
<<>>=
plot1 <- ggplot(FE2013) + 
          geom_point(aes(x=FEhighway, y  = FEcity), size=3.5)+
          xlab('Miles per Gallon: Highway')+
          ylab('Miles per Gallon: City')+
          ggtitle('Fuel Economy')
plot1
@
<<>>=
plot1 <- ggplot(FE2013) + 
          geom_point(aes(x=FEhighway, y  = FEcity), pch=24)+
          xlab('Miles per Gallon: Highway')+
          ylab('Miles per Gallon: City')+
          ggtitle('Fuel Economy')
plot1
@


We can also change the background theme and the font side.
<<>>=
plot1 + 
  theme_bw(20)
##theme_bw changes the color theme, 
##20 means 20pt font
@
<<>>=
##Also
plot1 + 
  theme_classic(20)
@
<<>>=
plot1 + 
  theme_gray(20)
@
<<>>=
plot1 + 
  theme_minimal(20)
@

For more information on shapes and colors that are available to \verb@ggplot@ see
\url{http://www.cookbook-r.com/Graphs/Shapes_and_line_types/}
and
\url{http://www.cookbook-r.com/Graphs/Colors_(ggplot2)/}
\section{Adding addition geoms}

We can easily add more things to our plot. In this example we'll add a best fit line, an arbitrary line, and a rug plot.
\begin{note}
In this plot we will specify \verb@aes@ in the the initalization step, this specifies it as a global option.  In other words it's the same as entering into each geom individually.
\end{note}
<<>>=
plot2 <- ggplot(FE2013, aes(x=FEhighway, y=FEcity))+
            geom_point()+ ##Since we used aes globally we don't need it here
            geom_smooth(method='lm', size=1)+ ##best fit line, size 1
            geom_abline(intercept=50, slope=-1, color='red', 
                        size=2)+ ##line
            geom_rug(sides='b')##just across the bottom
plot2     
@
In theory we could keep adding on and on.
\section{Special plots}
We'll now take a look at some other commonly used plots.  If you have the need for other types of plots I'd recommend looking at \url{http://www.cookbook-r.com/Graphs/} first.  They have many wonderful example with code.
\subsection{Histogram}
We'll start with histograms.
<<>>=
plot3 <- ggplot(FE2013, aes(x=FEcombined))+ ##only need x for hist
            geom_histogram(binwidth=1)

##if you don't specify binwidth it chooses something, 
##I specified it so you can see how
plot3
@
<<>>=
plot3 <- plot3 + 
            geom_histogram(binwidth=1, 
                           color='black', ##outline
                           fill='white')+ ##Inside
            ylab('Count')+
            xlab('Miles per Gallon: Combined')
plot3
@
We can change counts in the histogram to density
<<>>=
plot3 <- ggplot(FE2013, aes(x=FEcombined))+ ##only need x for hist
            geom_histogram(binwidth=1, 
                           color='black', ##outline
                           fill='white', ##Inside
                           aes(y=..density..))+##call aes again
            ylab('Density')+
            xlab('Miles per Gallon: Combined')


plot3
@
\subsection{Density}
We'll now look at density plots
<<>>=
plot4 <- ggplot(FE2013, aes(x=FEcombined))+ ##only need x for hist
            geom_density()+
            ylab('Density')+
            xlab('Miles per Gallon: Combined')+
            ggtitle("Basic Density")

plot4
@
We can see that it matches by overlapping them
<<>>=
plot4 <- plot3 + 
          geom_density()+
            ylab('Density')+
            xlab('Miles per Gallon: Combined')+
            ggtitle("Density + Hist.")

plot4
@
We can also do multiple densities at the same time
<<>>=
plot5 <- ggplot(FE2013) + 
          geom_density(aes(x=FEcity ,
                           fill = "City",
                           color= "City"), 
                       alpha = 0.5)+
          geom_density(aes(x=FEhighway ,
                           fill = "Highway",
                           color= "Highway"), 
                       alpha = 0.5)+
          ylab('Density')+
          xlab('Miles per Gallon')+
          ggtitle("Two Densities")+
          guides(fill = guide_legend(title = 'Type'),
                 color = guide_legend(title = 'Type'))##Change legend title
plot5
@
\begin{note}
In the last example we specified fill and color as strings, and \verb@ggplot@ made the legend for us.  It is also possible to specify them as a variable (like we did with Cylinder above, and ggplot will still make the lenged for us.)
\end{note}

\section{Stacking Plots}
We can arrange multiple plots on a single page using the gridExtra package
<<>>=
library(gridExtra)
grid.arrange(plot3, plot4, plot5, ncol=2)
@
You may not like the look of this so we can adjust that bottom plot
<<>>=
grid.arrange(arrangeGrob(plot3, plot4, nrow=1),
             plot5,
             nrow=2)
@
\section{Saving Plots}
To save individual plots you can do the following
<<>>=
ggsave(plot4, file="plot4.pdf", height=4, width=6)
@
To save either individual or pages of plots you can use pdf
<<>>=
pdf("plot4.pdf", height=4, width=6)
plot4
dev.off()
pdf("FullPlots.pdf", height=10, width=15)
grid.arrange(arrangeGrob(plot3, plot4, nrow=1),
             plot5,
             nrow=2)
dev.off()
@
You don't have to save as .pdf, that's just want I always do because it's easy to use them
for \LaTeX\ figures. You could save as .bmp, .jpeg, .png, or .tiff using the same approach. We'll
return to plotting in the next chapter when we discuss how to plot the substantive effects
from regression models.


<<echo=FALSE, results='hide'>>=
knitr::opts_chunk$set(comment=NA)
knitr::opts_chunk$set(prompt=TRUE, tidy=FALSE)
options(keep.blank.lines= TRUE)
@


\chapter{Additional Information and Useful functions in \R}
Today we'll look at running statistical models in \R.  Today we'll cover everything from cross-tabs to MLE estimation in \R.  To start we'll take a look at 2 variable contingency tables and correlation coefficients.
Please make sure that you have the following packages installed.
\begin{itemize}
  \item car
  \item lmtest
  \item MASS
  \item plm
  \item xtable
  \item stargazer
\end{itemize}
\section{Bivariate relationships}
We'll start by generating some related data.  Let's generate a 2 column matrix composed of draws from the Multivariate Normal, i.e.
$$x \sim N\left(
                  (0, 0), 
                  \begin{bmatrix}
                  1 & 0.4\\
                  0.4 & 1
                  \end{bmatrix}
          \right)$$
To do that we need to use the function \verb@MASS::mvrnorm@          
<<>>=
library(MASS)
x <- mvrnorm(1000, ##number of draws
             mu=c(0, 0), ##Mean vector
             Sigma=matrix(c(1,  ##Var matrix
                            0.4,
                            0.4,
                            1),
                          nrow=2))

head(x)

##Should be about the same by construction
var(x) ##Variance-Covariance matrix
cor(x) ##Correlation matrix

##If you just want the correlation coef.
cor(x[,1], x[,2]) ##Pearson's rho

@
To do cross tabs we'll need to convert to categorical data.
<<>>=
x[x>0] <- TRUE
x[x<0] <- FALSE
tab1 <- table(x[,1], x[,2])  ##Work great with factors
tab1
chisq.test(tab1)

##Both methods can handle more than two variables
x <- runif(1000)
y <- rnorm(1000)
z <- rpois(1000, lambda=1)

cor(x, y)
cor(cbind(x,y,z))

x[x<1/3] <-0
x[x>1/3 & x < 2/3] <- 0.5
x[x>2/3] <- 1
x <- factor(x, labels=c('Left',
                        'Middle',
                        'Right'))
table(x)
y <- ifelse(y>0, "Up", "down")
table(y, x)
chisq.test(table(y,x))
@
But setting that aside for now let's get into estimating models
\section{OLS: lm}
For this section we'll read in two data sets.
<<>>=
library(foreign)
FearonLaitin <- read.dta("Regression Data/FearonLaitin_CivilWar2003.dta")
Wages <- read.dta("Regression Data/wages_full_time.dta")
UNSC <- read.dta("Regression Data/KW_bare.dta")
##Check the var names
colnames(FearonLaitin)
colnames(Wages)
colnames(UNSC)

##and for demonstration purposes
X <- cbind(1, rnorm(1000), rnorm(1000, 1, 2))
b <- c(1, -2, 2)
y <- X %*% b + rnorm(1000)
@
OLS in \R\ is down with the \verb@lm@ command.  The \verb@lm@ command has the following options
\begin{description}
  \item[Formula:]  The formulas takes the following form: 
  \begin{verbatim}
          Y ~ X1 + X2 ...
  \end{verbatim}
  Where \verb@Y@ is the dependent variable and the \verb@X@s are whatever independent variables we want to include in the model.  The tilde is used to separate them.  We can also include a \verb@ -1 @ if we want to drop the constant term.
  \item[data:] An argument that tells \R\ what data frame we want to use.
  \item[subset:] An argument that takes logical statements.  It is used to restrict the model to a certain subset of the data.
  \item[weights:] If you want to specify weights (i.e. Weighted Least Squares) you can put the vector of weights here
  \item[model, x, y:]  These are arguments that tell \R\ you want it to also return the data used to fit the model (useful for plotting results)
\end{description}
To run a model then we can just to the following:
<<>>=
##Ordinary model
model1 <- lm(Wage~Male+Age, data=Wages) 

##Run only on Males
model2 <- lm(Wage~Age, data=Wages, subset=Male==1)
summary(model1)
summary(model2)

##We can also make adjustments in the formula
##Use I() to make most adjustments
model3 <- lm(log(Wage)~Male + Age +I(Age^2), data=Wages) 
summary(model3)

##And create interactions
model4 <- lm(log(Wage)~Male*Age, data=Wages) 
summary(model4)

## Interactions with no constituents
model5 <- lm(log(Wage)~Male:Age, data=Wages) 
summary(model5)
@
An lm object contains a bunch of information, use the \verb@names@ command to see what all it contains
<<>>=
names(model1)

model1$coefficients ##coefs
head(model1$residuals)  ##residuals
head(model1$fitted.values) ##XB
head(model1$model) ##data used to fit the model
model1$call ##Returns the command used to create it

vcov(model1) ##returns Variance matrix of model

model1 <- lm(formula = Wage ~ Male + Age, data = Wages, x=TRUE, y=TRUE)
head(model1$x) ##X values used
head(model1$y) ##y values used

##Check results 
##notice we can abbreviate elements of the lm
summary(cbind(model1$x %*% model1$coef,
              model1$fitted))
summary(cbind(model1$y-model1$fitted,
              model1$resid))

##If the data is in matrix form then we still use the formula
##Just not the data argument

##We need -1 because we have our own constant
summary(lm(y~X -1 ))  
@
To get fitted values with standard errors we can use the predict command.
<<>>=
modelFit <- predict(model1, se.fit=TRUE)
#This returns a list \hat{y} and s.e.(\hat{yat})

#Create a profile of data that is of interest to us.
profile <- data.frame(Male = 1,
                      Age = seq(18, 80, length.out=15))
fitted <- predict(model1, se.fit=TRUE, newdata=profile)
fitted
@



\subsection{Robust Standard Errors and Hypothesis Testing}
Suppose we wanted robust standard errors, there are actually a few ways to do this.  
Most common is the following
<<>>=
library(car)
library(lmtest)

###car::hccm returns the robust covariance matrix
RobustVar <- hccm(model1) 
RobustVar

##use lmtest::coeftest to get the t test
coeftest(model1, RobustVar)
@
\texttt{coeftest} is a great function where you can use new covariance matrices on your models.  


The car package also offers a function for joint hypothesis testing
<<>>=
summary(model3)

##Same as the basic t-test
linearHypothesis(model3, c("Age=0"))


##Special test
linearHypothesis(model3, c("Age=2"))

##Test if they have the same coefficient
linearHypothesis(model3, c("Age=I(Age^2)"))

##Joint significance of Age
linearHypothesis(model3, c("Age=0", "I(Age^2)=0"))

##Full F test 
linearHypothesis(model3, c("Age=0", "I(Age^2)=0", "Male=0"))

###Can also test with robust errors
##notice we're calling the hccm function itself here.
## R is smart enough to figure that out
linearHypothesis(model3, 
                 c("Age=0", "I(Age^2)=0", "Male=0"),
                 vcov=hccm) 

## Also can just give it a matrix (like from a bootstrap)
linearHypothesis(model3, 
                 c("Age=0", "I(Age^2)=0", "Male=0"),
                 vcov=hccm(model3)) 


###In the same vein there's also a function for nested model testing###
model6 <- lm(log(Wage)~Male , data=Wages) 

##Same as joint Significance test on Age above 
##This is an example of nested model testing
waldtest(model3, model6)
@

\subsection{Fixed Effects and Clustered Errors}
To estimate fixed effects we have some options. The first is to use  dummies
<<>>=
FEmodel1 <- lm(ln_totaid96~scmem+factor(ccode), data=UNSC)
summary(FEmodel1)

##This is an eyesore.  We can use plm 
library(plm)


FEmodel2 <- plm(ln_totaid96~scmem,
                model="within", ##FE command
                index=c("ccode", "year"), ##panel vars
                data=UNSC)
summary(FEmodel2)

##Note there's no intercept.  
##Stata's intercept for the FE model
##Is the average of the fixed effects, to get that
mean(fixef(FEmodel2))


##Random effects
REmodel1 <- plm(ln_totaid96~scmem,
                model="random", ##RE command
                index=c("ccode", "year"), ##panel vars
                data=UNSC)

summary(REmodel1)


##Hausman test
phtest(FEmodel2, REmodel1)

##Clustered errors
##Same as running lm (plm has built in clustering functions)
pool1 <- plm(ln_totaid96~scmem,
             model="pooling",
             index=c("ccode", "year"),
             data=UNSC)

## Panel Corrected Standard Errors
PCSE <- vcovBK(pool1, cluster="time")
coeftest(pool1, PCSE)

##Can use LinearHypothesis with this new Variance matrix
##And can use either F or Chi2 distribution
linearHypothesis(pool1, "scmem=0", vcov=PCSE, test="F")
linearHypothesis(pool1, "scmem=0", vcov=PCSE, test="Chisq")


##Fixed effects model SE clustered on country
clust <- vcovBK(FEmodel2, cluster="group")
coeftest(FEmodel2, clust)
@
\subsection{A smorgasbord of Regression Tests and Statistics}
Additional regression tests and statistics are listed here as a reference
\begin{table}[H]
\centering
\caption{Additional Tools and Tests}
\begin{tabular}{lll}
Function         & Use                                                               & Package \\ \hline
t.test           & Test if two samples drawn from normals with the same mean         & stats\\
wilcox.test      & Test if two samples drawn from distribution with same mean        & stats\\
chisq.test       & Pearson's chi square test for independence or goodness-of-fit     & stats\\
fisher.test      & Fisher's exact test for independence in 2x2 tables                & stats\\
ks.test          & Kolmogorov-Smirnov test for comparing distributions               & stats\\
phtest           & Hausman's test for random effects v fixed effects                 & plm\\
aic              & Returns the AIC of a model                                        & stats\\
bic              & Returns the BIC of a model                                        & stats\\
clarke           & Clarke's test for (non-nested) model comparison                   & games\\
vuong            & Voung's test for (non-nested) model comparison                    & games\\\
lrtest           & Likelihood ratio  test for (nested) model comparison              & lmtest\\
linearHypothesis & Wald test for (nested) model comparison                            & car\\
Box.test         & Box-Pierce \& Ljung-Box tests for autocorrelation                 & stats\\
bptest           & Breusch-Pagan test for heteroskedasticity                         & lmtest\\
durbinWatsonTest & Durbin-Watson test for autocorrelation in Errors                  & car\\
shapiro.test     & Shapiro-Wilk test for normality                                   & stats\\ 
jarque.bera.test & Jarque-Bera test for normality                                    & tseries\\ \hline

\end{tabular}
\end{table}
\section{GLMs}
There's only so much you can do with OLS and so we'll set that aside and look GLMs and MLE models.
GLMs in \R\ are pretty straight forward and can be estimated using the \verb@glm@ function.
These are models that you'll get to in 505, so for now just note that they're here if you need them.
<<>>=
model11 <- glm(onset ~ ethfrac + relfrac, 
              family=binomial, ##specify logit
              data=FearonLaitin)
summary(model11)

names(model11)

##Everything from lm carries over

model12 <- glm(onset ~ ethfrac + relfrac + log(gdpen), 
              family=binomial,
              data=FearonLaitin)
model13 <- glm(onset ~ ethfrac + relfrac+log(gdpen) + pop + I(pop^2),
              data=FearonLaitin,
              family=binomial, ##specify logit
              x=TRUE, y=TRUE)

##Subsetting
model14 <- glm(onset ~ ethfrac + relfrac+log(gdpen),
              data=FearonLaitin,
              family=binomial, ##specify logit
              x=TRUE, y=TRUE,
              subset=!is.na(pop))

##Check the fitted values
##plogis is the CDF of the logistic distribution
summary(cbind(plogis(model13$x %*% model13$coef),
              model13$fitted.values))


##Note that glms also have an option for convergence 
model14$converged
@

GLMs can take any of the following models\\[12pt]
\begin{center}
\begin{tabular}{rl}
Model  & \texttt{family=} \\ \hline
logit  & \texttt{binomial}\\
probit & \texttt{binomial(link="probit")}\\
cloglog& \texttt{binomial(link="cloglog")}\\
OLS    & \texttt{gaussian}\\
Poisson& \texttt{poisson}\\
gamma  & \texttt{Gamma}\\ \hline
\end{tabular}
\end{center}

\section{Tabling Results}
Now that we have some models we may want to put them in tables so that we can put them in a paper.
All of the functions that I know for this are convert \R\ output into either HTML or \LaTeX.  I don't know of an easy way to convert either to Word.  We'll look at 2 packages to convert \R\ to \LaTeX, although many others exist.
\subsection{\texttt{xtable}}
The first tabling package we'll look at is \verb@xtable@.
<<>>=
library(xtable)
xtable(model1)
@
Which in \LaTeX\ looks like
<<results='tex'>>=
xtable(model1)
@
Now we can customize this using built in arguments and using the \verb@print@ command.
<<>>=
##Give the variables nice names
names(model1$coefficients) <- c("Intercept", 
                                "Ethnic Frac.", 
                                "Religous Frac.")

print(xtable(model1, 
             caption="Fearon and Laitin Logit", 
             label="tab:FL1",
             align=c("rcccc"),
             digits=2),
      caption.placement="top",
      table.placement="htb")

##We can write it to a .tex file rather than copy/paste
print(xtable(model1, 
             caption="Fearon and Laitin Logit", 
             label="tab:FL1",
             align=c("rcccc"),
             digits=2),
      caption.placement="top",
      table.placement="htb",
      file="Table1.tex")


####Print model 1 using robust standard errors
robustModel1 <- coeftest(model1, RobustVar)
robustModel1 <- coeftest(model1, RobustVar)[] 

##For quirky reasons only the command with [] can be 
##given to xtable
##Note that using [] extracts just the matrix part of it

xtable(robustModel1)


@
To read it in from a file we can use the \verb@\input@ command in \LaTeX.
So 
\begin{verbatim}
\input{Table1.Tex}
\end{verbatim}
gives us Table \ref{tab:FL1} below.
\input{Table1.tex}




MANY  options exist to customize \verb@xtable@ output, and we could spend hours going over just different commands within \verb@xtable@.  Take a look at help file by running ?print.xtable and go to
\url{http://cran.r-project.org/web/packages/xtable/vignettes/xtableGallery.pdf} to see examples (with code) of customized \verb@xtable@ output.
\par

\subsection{\texttt{stargazer}}
There are a lot of times when we would prefer to view models side by side.  For this I recommend  \verb@stargazer::stargazer@.  Unlike \verb@xtable@ it works on nearly every canned model directly (to varying degrees of effectiveness so watch out).
<<>>=
library(stargazer)
stargazer(model2, model3, FEmodel2,
          title="Trying Stargazer",
          label="tab:Star",
          ##It allows you to put nice names 
          ##directly into the call
          covariate.labels=c("Ethnic Frac.",
                             "Religious Frac.",
                             "log(GDP per capita)",
                             "Population",
                             "Population$^2$",
                             "UNSC Member"),
          ##nice names for the Dependent
          ##Variables
          dep.var.labels=c("Onset", "Foreign Aid"),
          digits=2,
          ##Emulates many different journals including
          ##APSR, AJPS, IO, and AER
          style= "AJPS")

####You can write it to a .tex file using the out option
@
<<results='tex'>>=
stargazer(model2, model3, FEmodel2,
          title="Trying Stargazer",
          label="tab:Star",
          out="StarTable.tex", #write to file
          digits=2,
          ##It allows you to put nice names 
          ##directly into the call
          covariate.labels=c("Ethnic Frac.",
                             "Religious Frac.",
                             "log(GDP per capita)",
                             "Population",
                             "Population$^2$",
                             "UNSC Member"),
          ##nice names for the Dependent
          ##Variables
          dep.var.labels=c("Onset", "Foreign Aid"),
          ##Emulates many different journals including
          ##APSR, AJPS, IO, and AER
          style= "AJPS")

@
There is lots of customization available for \verb@stargazer@ output as well, which can all be found in the help file for the function.
\section{Other Models and Where to Find Them}
\begin{table}[H]
\centering
\caption{Other Common Models and their  Packages}
\label{models}
\begin{tabular}{ll}
Model                          &  Package \\ \hline
Conditional Logit              &  survival\\
Weibull, Exponential, Cox PH   &  survival\\
Ordered Probit                 &  MASS\\
Negative Binomial              &  MASS\\
Multinomial Logit              &  VGAM or mlogit\\
Heckman Selection              &  sampleSelection\\
Strategic Models               &  games\\
Adaptive Lasso (OLS \& logit)  &  polywog\\
Matching                       &  MatchIt and cem\\
GAMS                           &  mgcv\\ 
ARIMA models                   &  stats (loads automatically)\\
Instrumental Regression        &  AER\\
tobit                          &  AER\\
Bayesian GLM                   &  arm\\
\hline
\end{tabular}
\end{table}
Note that for \texttt{lm} and \texttt{glm} models you can get clustered standard errors by using the ``multiwayvcov'' package.  Survival models have a cluster option available as well.  
Other models can be estimated by maximizing a likelihood (using \texttt{optim}) or by Bayesian methods using rjags or stan.

\section{Plotting Results from Regression Models}
While these summary plots from the last chapter are interesting on their own, the real reason you want to learn to plot is to plot regression results.  So we'll look at some data on GRE scores and admission into grad school to look at some interesting ways to represent regression results graphically.
<<>>=
##Data
GREdat <- read.csv("http://www.ats.ucla.edu/stat/data/binary.csv")
colnames(GREdat)
@
Suppose then we want to predict admission to grad school using gre and the ranking of the candidates undergraduate institution (4 pt scale). As I'm sure you can guess, that's exactly what's in the data we just read in. Our model then is
<<>>=
admitMod <- glm(admit ~ gre  + factor(rank), 
                data=GREdat,##data
                family=binomial, ##
                x=TRUE) ##We'll want the X matrix

summary(admitMod)


###And a parametric bootstrap
library(MASS)
admitBoot <-mvrnorm(5000, mu=admitMod$coef, Sigma=vcov(admitMod)) 

@
%%use ucla, introduce faceting, use rank for limited
\subsection{Continuous Variables}
Let's say we're first interesting in the effect that GRE score has on admission.  We first want to hold all the other variables constant.
<<>>=
library(ggplot2)
###Store all variables to their median
X <- replicate(20, apply(admitMod$x, 2, median))
X <- t(X)
X[,"gre"] <- seq(min(GREdat$gre),
                 max(GREdat$gre),
                 length.out=20)

X

XB <- plogis(X %*% t(admitBoot))


plottingData <- data.frame(fit = apply(XB, 1, mean),
                           hi = apply(XB, 1, quantile, 0.975),
                           lo = apply(XB, 1, quantile, 0.025),
                           GRE = X[,"gre"])
plottingData

########plot it ########
grePlot <- ggplot(plottingData)+
              geom_line(aes(x=GRE, y=fit))+
              geom_ribbon(aes(x=GRE, ymin=lo, max=hi), alpha=0.25)+
              ylab("Fitted Probability")+
              xlab("GRE")+
              ggtitle("Predicting Admission\n using GRE scores")+
              theme_bw(18)
grePlot
@

We could in theory also examine how it changes with by changing rank
<<>>=
##Make X2 with rank2 = 1 
X2 <- X
X2[, "factor(rank)2"]  <- 1

##Same with 3 and 4
X3 <- X
X3[, "factor(rank)3"]  <- 1

X4 <- X
X4[, "factor(rank)4"]  <- 1

X <- rbind(X, X2, X3, X4)

XB <- plogis(X %*% t(admitBoot))


plottingData <- data.frame(fit = apply(XB, 1, mean),
                           hi = apply(XB, 1, quantile, 0.975),
                           lo = apply(XB, 1, quantile, 0.025),
                           GRE = X[,"gre"],
                           Rank = rep(1:4, each=20))

##Rank here indicates which Rank we used it changes every 20 observations
##Need to convert to factor for indexing the plot
plottingData$Rank <- factor(plottingData$Rank)


grePlot2 <- ggplot(plottingData)+
              geom_line(aes(x=GRE, 
                            y=fit, 
                            color=Rank))+
              geom_ribbon(aes(x=GRE, 
                              ymin=lo,
                              max=hi,
                              fill=Rank), alpha=0.25)+
              ylab("Fitted Probability")+
              xlab("GRE")+
              ggtitle("Predicting Admission\n using GRE scores")+
              theme_classic(18)
grePlot2
@
But we may say that there's too much overlap and we could split it into separate plots.  Either by creating four different plots or by using the facet option
<<fig.width=10, fig.height=8>>=
##Give better labels
levels(plottingData$Rank) <- c("Rank: 1",
                               "Rank: 2",
                               "Rank: 3",
                               "Rank: 4")

ggplot(plottingData)+
              geom_line(aes(x=GRE, 
                            y=fit))+
              geom_ribbon(aes(x=GRE, 
                              ymin=lo,
                              max=hi), alpha=0.25)+
              facet_wrap(~Rank, ncol=2)+ ##Can specify columns
              ylab("Fitted Probability")+
              xlab("GRE")+
              ggtitle("Predicting Admission\n using GRE scores")+
              theme_classic(18)
@
\subsection{Categorical Variables}
However suppose we wanted to know the effect of rank on the different values. These proceeds in much the same way.
<<>>=
X <- replicate(4, apply(admitMod$x, 2, median))
X <- t(X)

X
##Need to let Ranks vary.  Not clear what the best way to do this is
##We want to replace the bottom three rows the the rank variable
##With length 3 Identity matrix.  I purpose using the 
##stringr package to create a logical that returns true if the word
##rank is in the column name

library(stringr) ##functions for character vectors
str_detect(colnames(X), "rank")

##returns a logical that equals TRUE if rank is in the column name

X[2:4, str_detect(colnames(X), "rank")] <- diag(3)
X

XB <- plogis(X %*% t(admitBoot))


plottingData <- data.frame(fit = apply(XB, 1, mean),
                           hi = apply(XB, 1, quantile, 0.975),
                           lo = apply(XB, 1, quantile, 0.025),
                           Rank = 1:4)
plottingData

rankPlot <- ggplot(plottingData)+
              geom_pointrange(aes(x=Rank,
                                    y=fit,
                                    ymin=lo,
                                    ymax=hi),
                              size=0.75)+
              theme_grey(18)+
              ylab('Fitted Probability')+
              ggtitle('Effect of Ungraduate Rank\n on Graduate Admission')
rankPlot
@



 

  

\appendix
\chapter{Answers to Exercises}
<<echo=FALSE, results='hide'>>=
knitr::opts_chunk$set(comment=NA)
knitr::opts_chunk$set(prompt=TRUE, tidy=FALSE)
options(keep.blank.lines= TRUE)
<<results='hide', echo=FALSE>>=
read_chunk("Answers/Chapter1Ans.R")
read_chunk("Answers/Chapter2Ans.R")
read_chunk("Answers/Chapter3Ans.R")
@
\section{Answers to Chapter 1 Exercises}
<<p1, eval=FALSE>>=
@
<<p2>>=
@
<<p3, eval=FALSE>>=
@
\section{Answers to Chapter 2 Exercises}
<<>>=
#Problem 1
maximum <- function(x){
  N <- length(x)
  currentMax <- x[1]
  for(i in 2:N){
    if(currentMax <= x[i]){
      currentMax <- x[i]
    }
  }
  return(currentMax)
}

#test it 
results <- logical(1000)
for(i in 1:1000){
  x <- rnorm(100, 0, 1000) #100 draws from diffuse normal
  results[i] <- maximum(x)==max(x)
}
all(results)
@
<<>>=
#Problem 2

#Part a 
library(geoR)
library(MASS)
N <- 2000
X <- cbind(1, replicate(2,rnorm(N))) #gen X
beta <- c(-1, 2, -2) #true betas
sigma2 <- 4 #true sigma2
y <- X %*% beta + rnorm(N, 0, sqrt(sigma2)) #DGP
results <- matrix(0, 10000, 4) #results matrix (part b)
beta_i <- runif(3, -100000, 100000) #starting value (part c)
mu <- solve(t(X) %*% X) %*% t(X) %*% y #mean of beta

for(i in 1:10000){ #part d
  betaNew <- beta_i #set current beta (d.i)
  s2 <- 1/(N-3) * (t(y- X%*%betaNew) %*% (y- X%*%betaNew)) #scale for inv chi2
  sigma_i <- as.numeric(rinvchisq(1, N-3, scale=s2)) #draw sigma (d.ii)
  beta_i <- mvrnorm(1, mu, solve(t(X)%*%X)*sigma_i) #draw beta (d.iii)
  results[i,] <- c(beta_i, sigma_i) #store current estimate (d.iv)
}
results <- results[5001:10000,] #remove burn-in (e)
colMeans(results) #estimates (f)
@
<<>>=
#Problem 3
ols <- function(X, y){
  bhat <- solve(t(X) %*% X) %*% t(X) %*% y
  ehat <- y - X %*% bhat
  n <- nrow(X)
  p <- ncol(X)
  s <- t(ehat) %*% ehat/ (n - p)
  var <- as.numeric(s) * solve(t(X) %*% X)
  se <- sqrt(diag(var))
  ans <- list(coef = bhat,
              vcov = var,
              SE = se)
  return(ans)
}

ols(X, y)


###Bonus####

ols <- function(X, y){
  bhat <- solve(t(X) %*% X) %*% t(X) %*% y
  ehat <- y - X %*% bhat
  n <- nrow(X)
  p <- ncol(X)
  s <- t(ehat) %*% ehat/ (n - p)
  var <- as.numeric(s) * solve(t(X) %*% X)
  se <- sqrt(diag(var))
  
  t <- bhat/se
  df <- n -p  
  p <- pt(abs(t), lower.tail=FALSE, df=df)*2
  
  ans <- cbind(bhat, se, t, p)
  colnames(ans) <- c("Est", "SE", "t", "p")
  return(ans)
}

ols(X, y)
@
<<>>=
#Problem 4
grNormalMLE <- function(theta, X,y){
  eta <- theta[length(theta)] #extract eta from parameter vector
  beta <- theta[-length(theta)] #beta coefficients
  
  dBeta <- (X * as.numeric(y-X%*%beta))/exp(eta)
  dEta <- (y- X%*%beta)^2 / (2*exp(eta)) - 1/2
  D <- colSums(cbind(dBeta, dEta))
  return(-D)
}
@
\section{Answer to Chapter 3 Exercises}
<<>>=
library(foreign)
library(reshape2)
library(data.table)
library(stringr)
rm(list=ls())
@
<<>>=
##########Problem 1 #############
####problem 1a####
Fl2003 <- read.dta('Datasets/FearonLaitin_CivilWar2003.dta')
pressUse <- read.csv('Datasets/Press_FH.csv', encoding="latin1")  
#Note that I specified the encoding, this is because they have a weird 
#character in Cote d'Ivoire's name.  This fixes the problem on my computer.

@
<<>>=
####problem 1b####
pressUse <- melt(pressUse, 
                 id.vars="country",
                 variable="year",
                 value.name="press")
@

<<>>=
####problem 1c####
pressUse$year <- str_replace(pressUse$year, "X", "")
@
<<>>=
####problem 1d####
source('Datasets/stringr_to_cow.R')
pressUse <- stringr.to.cow(pressUse$country,
                           pressUse$year,
                           pressUse)
 
pressUse <- pressUse[pressUse$ccode != 0, ]
pressUse <- pressUse[pressUse$press != "N/A",]
@
% <<>>=
% ####problem 1e####
% mergedDat <- merge(Fl2003, pressUse,
%                    by=c("ccode", "year"),
%                    all.x=TRUE)
% nrow(mergedDat)==nrow(Fl2003)
% @
% <<>>=
% ####problem 1f####
% cDummies <- model.matrix(~factor(ccode)-1, data=Fl2003)
% cYear <- model.matrix(~factor(year)-1, data=Fl2003)
% 
% Fl2003 <- cbind(Fl2003, cDummies, cYear)
% @
\end{document}

